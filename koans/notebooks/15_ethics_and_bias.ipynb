{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics and Bias in Machine Learning - Data Science Koans\n",
    "\n",
    "Welcome to Notebook 15: Ethics and Bias - The Final Frontier!\n",
    "\n",
    "## What You Will Learn\n",
    "- Measuring fairness with quantitative metrics  \n",
    "- Detecting bias in model predictions and training data\n",
    "- Implementing bias mitigation techniques\n",
    "- Model interpretability and explainability methods\n",
    "- Creating responsible ML checklists and governance\n",
    "\n",
    "## Why This Matters More Than Ever\n",
    "As ML systems increasingly impact human lives, ethical considerations become paramount:\n",
    "- **Legal Compliance**: Avoid discrimination lawsuits and regulatory violations\n",
    "- **Social Responsibility**: Ensure equitable outcomes across all groups  \n",
    "- **Business Risk**: Prevent reputational damage from biased systems\n",
    "- **Trust Building**: Maintain public confidence in AI systems\n",
    "- **Fairness**: Uphold principles of justice and equality\n",
    "\n",
    "## Key Ethical Challenges\n",
    "- **Historical Bias**: Training data reflects past discrimination\n",
    "- **Representation Bias**: Underrepresented groups in datasets\n",
    "- **Measurement Bias**: Proxy variables that correlate with protected attributes\n",
    "- **Algorithmic Amplification**: ML systems that exacerbate existing inequalities\n",
    "- **Explainability**: Understanding why models make certain decisions\n",
    "\n",
    "## Prerequisites\n",
    "- Model Selection and Pipelines (Notebook 14)\n",
    "- Understanding of classification metrics\n",
    "- Awareness of social justice issues\n",
    "\n",
    "## Critical Mindset\n",
    "This notebook isn't just about technical implementation - it's about developing ethical reasoning skills that will guide your entire ML career. Every model you build affects real people.\n",
    "\n",
    "## How to Use\n",
    "1. Examine each ethical scenario with critical thinking\n",
    "2. Implement bias detection and mitigation techniques\n",
    "3. Practice explaining model decisions to diverse stakeholders  \n",
    "4. Develop frameworks for ongoing ethical evaluation\n",
    "5. Build habits of responsible ML development\n",
    "\n",
    "Ready to become a responsible ML practitioner? Let's build ethical AI! 🧭✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run first!\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For model interpretability\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print(\"✓ SHAP available for model interpretability\")\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"⚠️ SHAP not available (install: pip install shap)\")\n",
    "\n",
    "try:\n",
    "    from lime.lime_tabular import LimeTabularExplainer\n",
    "    LIME_AVAILABLE = True\n",
    "    print(\"✓ LIME available for model explanations\")\n",
    "except ImportError:\n",
    "    LIME_AVAILABLE = False\n",
    "    print(\"⚠️ LIME not available (install: pip install lime)\")\n",
    "\n",
    "from koans.core.validator import KoanValidator\n",
    "from koans.core.progress import ProgressTracker\n",
    "\n",
    "validator = KoanValidator(\"15_ethics_and_bias\")\n",
    "tracker = ProgressTracker()\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current progress: {tracker.get_notebook_progress('15_ethics_and_bias')}%\")\n",
    "print(\"\\n🧭 Entering the realm of responsible AI...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 15.1: Fairness Metrics - Measuring Bias Quantitatively  \n",
    "**Objective**: Calculate fairness metrics to detect bias across groups  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Fairness can be measured in multiple ways, and different metrics may conflict. Understanding these trade-offs is crucial for building equitable ML systems.\n",
    "\n",
    "**Key Concepts**: \n",
    "- **Demographic Parity**: Equal positive prediction rates across groups\n",
    "- **Equal Opportunity**: Equal true positive rates across groups  \n",
    "- **Equalized Odds**: Equal TPR and FPR across groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fairness_metrics():\n",
    "    \"\"\"\n",
    "    Create a synthetic hiring dataset and calculate fairness metrics.\n",
    "    \n",
    "    We'll simulate a biased hiring algorithm and measure different fairness criteria.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Fairness metrics for different demographic groups\n",
    "    \"\"\"\n",
    "    # Create synthetic hiring dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 2000\n",
    "    \n",
    "    # Protected attribute: gender (0=male, 1=female)  \n",
    "    gender = np.random.choice([0, 1], n_samples, p=[0.6, 0.4])\n",
    "    \n",
    "    # Create features that correlate with both gender and hiring decision\n",
    "    # This simulates real-world scenarios where bias exists\n",
    "    experience = np.random.normal(5, 2, n_samples)  # Years of experience\n",
    "    education = np.random.normal(3, 1, n_samples)   # Education score\n",
    "    \n",
    "    # Introduce bias: women tend to have slightly lower \"network scores\" \n",
    "    # (simulating old-boys-network effects)\n",
    "    network_score = np.where(gender == 0, \n",
    "                            np.random.normal(7, 2, n_samples),  # Men\n",
    "                            np.random.normal(6, 2, n_samples))  # Women\n",
    "    \n",
    "    X = np.column_stack([experience, education, network_score])\n",
    "    \n",
    "    # Create biased hiring decisions\n",
    "    # True qualification should depend mainly on experience and education\n",
    "    true_qualification = (0.4 * experience + 0.6 * education + \n",
    "                         np.random.normal(0, 1, n_samples)) > 3.5\n",
    "    \n",
    "    # But hiring decisions are biased by network score and gender\n",
    "    biased_score = (0.3 * experience + 0.4 * education + 0.3 * network_score - \n",
    "                   0.2 * gender + np.random.normal(0, 1, n_samples))\n",
    "    y_biased = biased_score > 4.5\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test, gender_train, gender_test = train_test_split(\n",
    "        X, y_biased, gender, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # TODO: Train biased model\n",
    "    model = None  # RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    # TODO: Make predictions\n",
    "    # y_pred = model.predict(X_test)\n",
    "    y_pred = np.zeros_like(y_test)  # Placeholder\n",
    "    \n",
    "    # TODO: Calculate fairness metrics by gender\n",
    "    # Group data by gender\n",
    "    male_mask = gender_test == 0\n",
    "    female_mask = gender_test == 1\n",
    "    \n",
    "    # Demographic Parity: P(Ŷ=1|A=0) vs P(Ŷ=1|A=1)\n",
    "    male_positive_rate = None    # np.mean(y_pred[male_mask])\n",
    "    female_positive_rate = None  # np.mean(y_pred[female_mask])\n",
    "    demographic_parity_diff = None  # male_positive_rate - female_positive_rate\n",
    "    \n",
    "    # Equal Opportunity: TPR for each group\n",
    "    male_tpr = None   # recall_score(y_test[male_mask], y_pred[male_mask])\n",
    "    female_tpr = None # recall_score(y_test[female_mask], y_pred[female_mask])\n",
    "    equal_opportunity_diff = None  # male_tpr - female_tpr\n",
    "    \n",
    "    # Overall accuracy by group\n",
    "    male_accuracy = None   # accuracy_score(y_test[male_mask], y_pred[male_mask])\n",
    "    female_accuracy = None # accuracy_score(y_test[female_mask], y_pred[female_mask])\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'male_positive_rate': male_positive_rate or 0,\n",
    "        'female_positive_rate': female_positive_rate or 0,\n",
    "        'demographic_parity_diff': demographic_parity_diff or 0,\n",
    "        'male_tpr': male_tpr or 0,\n",
    "        'female_tpr': female_tpr or 0,\n",
    "        'equal_opportunity_diff': equal_opportunity_diff or 0,\n",
    "        'male_accuracy': male_accuracy or 0,\n",
    "        'female_accuracy': female_accuracy or 0,\n",
    "        'n_male': np.sum(male_mask),\n",
    "        'n_female': np.sum(female_mask)\n",
    "    }\n",
    "\n",
    "@validator.koan(1, \"Fairness Metrics - Measuring Bias Quantitatively\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = calculate_fairness_metrics()\n",
    "    \n",
    "    assert results['model'] is not None, \"Model not trained\"\n",
    "    assert results['n_male'] > 0 and results['n_female'] > 0, \"Should have both gender groups\"\n",
    "    assert 0 <= results['male_positive_rate'] <= 1, \"Male positive rate should be probability\"\n",
    "    assert 0 <= results['female_positive_rate'] <= 1, \"Female positive rate should be probability\"\n",
    "    \n",
    "    print(\"✓ Fairness metrics calculated successfully!\")\n",
    "    print(f\"  - Test samples: {results['n_male']} male, {results['n_female']} female\")\n",
    "    \n",
    "    print(f\"\\n  📊 Bias Detection Results:\")\n",
    "    print(f\"    Demographic Parity:\")\n",
    "    print(f\"      Male hiring rate: {results['male_positive_rate']:.3f}\")\n",
    "    print(f\"      Female hiring rate: {results['female_positive_rate']:.3f}\")\n",
    "    print(f\"      Difference: {results['demographic_parity_diff']:+.3f}\")\n",
    "    \n",
    "    print(f\"\\n    Equal Opportunity:\")\n",
    "    print(f\"      Male TPR (recall): {results['male_tpr']:.3f}\")\n",
    "    print(f\"      Female TPR (recall): {results['female_tpr']:.3f}\")\n",
    "    print(f\"      Difference: {results['equal_opportunity_diff']:+.3f}\")\n",
    "    \n",
    "    print(f\"\\n    Overall Accuracy:\")\n",
    "    print(f\"      Male accuracy: {results['male_accuracy']:.3f}\")\n",
    "    print(f\"      Female accuracy: {results['female_accuracy']:.3f}\")\n",
    "    \n",
    "    # Interpret results\n",
    "    dp_threshold = 0.05  # 5% threshold for demographic parity\n",
    "    eo_threshold = 0.05  # 5% threshold for equal opportunity\n",
    "    \n",
    "    if abs(results['demographic_parity_diff']) > dp_threshold:\n",
    "        print(f\"\\n  🚨 Demographic Parity Violation Detected!\")\n",
    "        print(f\"     Difference ({results['demographic_parity_diff']:+.3f}) exceeds threshold (±{dp_threshold})\")\n",
    "    else:\n",
    "        print(f\"\\n  ✅ Demographic Parity: Within acceptable range\")\n",
    "        \n",
    "    if abs(results['equal_opportunity_diff']) > eo_threshold:\n",
    "        print(f\"\\n  🚨 Equal Opportunity Violation Detected!\")  \n",
    "        print(f\"     Difference ({results['equal_opportunity_diff']:+.3f}) exceeds threshold (±{eo_threshold})\")\n",
    "    else:\n",
    "        print(f\"\\n  ✅ Equal Opportunity: Within acceptable range\")\n",
    "    \n",
    "    print(f\"\\n  💡 Understanding Fairness Metrics:\")\n",
    "    print(f\"    • Demographic Parity: Equal selection rates\")\n",
    "    print(f\"    • Equal Opportunity: Equal true positive rates\")\n",
    "    print(f\"    • These metrics can conflict - choose based on context\")\n",
    "    print(f\"    • Legal requirements vary by jurisdiction\")\n",
    "    \n",
    "    print(f\"\\n  ⚖️ Fairness Trade-offs:\")\n",
    "    print(f\"    • Perfect fairness across all metrics is often impossible\")\n",
    "    print(f\"    • Business context determines which metric to prioritize\")\n",
    "    print(f\"    • Regular monitoring is essential\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 15.2: Bias Detection - Identifying Problematic Patterns\n",
    "**Objective**: Systematically detect bias in datasets and model predictions  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Bias can hide in datasets through historical discrimination, sampling issues, or proxy variables. Systematic detection is the first step toward fair ML.\n",
    "\n",
    "**Key Concepts**: Examine data distributions, correlation with protected attributes, and model behavior across different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_bias_detection():\n",
    "    \"\"\"\n",
    "    Perform comprehensive bias detection on a loan approval dataset.\n",
    "    \n",
    "    We'll examine multiple sources of bias:\n",
    "    1. Dataset representation bias\n",
    "    2. Historical bias in labels  \n",
    "    3. Proxy variable bias\n",
    "    4. Model prediction bias\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive bias detection results\n",
    "    \"\"\"\n",
    "    # Create synthetic loan dataset with known biases\n",
    "    np.random.seed(42)\n",
    "    n_samples = 3000\n",
    "    \n",
    "    # Protected attributes\n",
    "    race = np.random.choice(['White', 'Black', 'Hispanic', 'Asian'], \n",
    "                           n_samples, p=[0.6, 0.15, 0.15, 0.1])\n",
    "    age = np.random.normal(40, 12, n_samples)\n",
    "    gender = np.random.choice(['Male', 'Female'], n_samples, p=[0.55, 0.45])\n",
    "    \n",
    "    # Create features with embedded bias\n",
    "    income = np.random.lognormal(10.5, 0.8, n_samples)\n",
    "    \n",
    "    # Introduce racial bias in income (historical discrimination)\n",
    "    race_multipliers = {'White': 1.0, 'Asian': 1.1, 'Black': 0.8, 'Hispanic': 0.85}\n",
    "    for i, r in enumerate(race):\n",
    "        income[i] *= race_multipliers[r]\n",
    "    \n",
    "    credit_score = np.random.normal(700, 100, n_samples)\n",
    "    # Introduce bias - credit scores correlate with race due to systemic factors\n",
    "    for i, r in enumerate(race):\n",
    "        if r == 'Black':\n",
    "            credit_score[i] -= 50\n",
    "        elif r == 'Hispanic':\n",
    "            credit_score[i] -= 30\n",
    "    \n",
    "    # Create biased loan approvals (historical bias)\n",
    "    # Approval should depend on income and credit score, but historically was biased\n",
    "    loan_worthiness = (0.3 * (income / 100000) + 0.7 * (credit_score / 700)) > 0.8\n",
    "    \n",
    "    # Historical bias in approvals\n",
    "    approvals = loan_worthiness.copy()\n",
    "    for i, r in enumerate(race):\n",
    "        if r in ['Black', 'Hispanic'] and np.random.random() < 0.2:\n",
    "            approvals[i] = False  # 20% additional rejection bias\n",
    "    \n",
    "    # Create dataset\n",
    "    df = pd.DataFrame({\n",
    "        'race': race,\n",
    "        'age': age,\n",
    "        'gender': gender,\n",
    "        'income': income,\n",
    "        'credit_score': credit_score,\n",
    "        'approved': approvals\n",
    "    })\n",
    "    \n",
    "    # TODO: Detect representation bias\n",
    "    representation_analysis = {}\n",
    "    for group in ['race', 'gender']:\n",
    "        # Calculate group proportions\n",
    "        proportions = None  # df[group].value_counts(normalize=True)\n",
    "        representation_analysis[group] = proportions\n",
    "    \n",
    "    # TODO: Detect outcome bias by protected attributes  \n",
    "    outcome_bias = {}\n",
    "    for group in ['race', 'gender']:\n",
    "        # Calculate approval rates by group\n",
    "        approval_rates = None  # df.groupby(group)['approved'].mean()\n",
    "        outcome_bias[group] = approval_rates\n",
    "    \n",
    "    # TODO: Train model and detect prediction bias\n",
    "    # Prepare features (exclude protected attributes from training)\n",
    "    X = df[['age', 'income', 'credit_score']]\n",
    "    y = df['approved']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # TODO: Train model\n",
    "    model = None  # LogisticRegression(random_state=42)\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    # TODO: Analyze model predictions by race\n",
    "    df_test = df.iloc[y_test.index] if hasattr(y_test, 'index') else df.iloc[-len(y_test):]\n",
    "    # y_pred = model.predict(X_test) if model else np.zeros(len(y_test))\n",
    "    y_pred = np.zeros(len(y_test))  # Placeholder\n",
    "    \n",
    "    df_test = df_test.copy()\n",
    "    df_test['predicted'] = y_pred\n",
    "    \n",
    "    # TODO: Calculate prediction bias by race\n",
    "    prediction_bias = {}\n",
    "    for race_group in df_test['race'].unique():\n",
    "        mask = df_test['race'] == race_group\n",
    "        if np.sum(mask) > 0:\n",
    "            pred_rate = None  # df_test[mask]['predicted'].mean()\n",
    "            actual_rate = None  # df_test[mask]['approved'].mean()\n",
    "            prediction_bias[race_group] = {\n",
    "                'predicted_approval_rate': pred_rate or 0,\n",
    "                'actual_approval_rate': actual_rate or 0\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'representation_analysis': representation_analysis,\n",
    "        'outcome_bias': outcome_bias,\n",
    "        'prediction_bias': prediction_bias,\n",
    "        'model': model,\n",
    "        'dataset_size': len(df)\n",
    "    }\n",
    "\n",
    "@validator.koan(2, \"Bias Detection - Identifying Problematic Patterns\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = comprehensive_bias_detection()\n",
    "    \n",
    "    assert results['representation_analysis'] is not None, \"Representation analysis not performed\"\n",
    "    assert results['outcome_bias'] is not None, \"Outcome bias analysis not performed\"\n",
    "    assert results['dataset_size'] > 0, \"Dataset not created properly\"\n",
    "    \n",
    "    print(\"✓ Comprehensive bias detection completed!\")\n",
    "    print(f\"  - Dataset size: {results['dataset_size']} samples\")\n",
    "    \n",
    "    print(f\"\\n  📊 Representation Analysis:\")\n",
    "    if 'race' in results['representation_analysis']:\n",
    "        race_dist = results['representation_analysis']['race']\n",
    "        if race_dist is not None:\n",
    "            for group, prop in race_dist.items():\n",
    "                print(f\"    {group}: {prop:.3f} ({prop*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n  🎯 Historical Outcome Bias:\")\n",
    "    if 'race' in results['outcome_bias']:\n",
    "        race_outcomes = results['outcome_bias']['race']\n",
    "        if race_outcomes is not None:\n",
    "            for group, rate in race_outcomes.items():\n",
    "                print(f\"    {group} approval rate: {rate:.3f}\")\n",
    "    \n",
    "    print(f\"\\n  🤖 Model Prediction Bias:\")\n",
    "    for group, metrics in results['prediction_bias'].items():\n",
    "        pred_rate = metrics['predicted_approval_rate']\n",
    "        actual_rate = metrics['actual_approval_rate']\n",
    "        print(f\"    {group}:\")\n",
    "        print(f\"      Predicted: {pred_rate:.3f}\")\n",
    "        print(f\"      Actual: {actual_rate:.3f}\")\n",
    "        print(f\"      Bias: {pred_rate - actual_rate:+.3f}\")\n",
    "    \n",
    "    print(f\"\\n  🔍 Bias Detection Checklist:\")\n",
    "    print(f\"    ✅ Representation bias analysis\")\n",
    "    print(f\"    ✅ Historical outcome analysis\")\n",
    "    print(f\"    ✅ Protected attribute correlation\")\n",
    "    print(f\"    ✅ Model prediction fairness\")\n",
    "    \n",
    "    print(f\"\\n  🚨 Common Bias Sources:\")\n",
    "    print(f\"    • Historical discrimination in training data\")\n",
    "    print(f\"    • Underrepresentation of minority groups\")\n",
    "    print(f\"    • Proxy variables (zip code → race)\")\n",
    "    print(f\"    • Sampling bias in data collection\")\n",
    "    print(f\"    • Confirmation bias in labeling\")\n",
    "    \n",
    "    print(f\"\\n  💡 Next Steps After Detection:\")\n",
    "    print(f\"    • Quantify the severity of bias\")\n",
    "    print(f\"    • Understand root causes\")\n",
    "    print(f\"    • Implement mitigation strategies\")\n",
    "    print(f\"    • Monitor ongoing bias\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 15.3: Bias Mitigation - Reducing Unfair Discrimination\n",
    "**Objective**: Implement techniques to reduce bias in ML models  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Once bias is detected, several techniques can help mitigate it. Each approach has trade-offs between fairness, accuracy, and complexity.\n",
    "\n",
    "**Key Techniques**: Data rebalancing, adversarial debiasing, fairness constraints, and threshold adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_bias_mitigation():\n",
    "    \"\"\"\n",
    "    Implement multiple bias mitigation techniques and compare their effectiveness.\n",
    "    \n",
    "    Techniques:\n",
    "    1. Data rebalancing (preprocessing)\n",
    "    2. Threshold adjustment (postprocessing)\n",
    "    3. Fairness-aware training (in-processing)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results comparing different mitigation approaches\n",
    "    \"\"\"\n",
    "    # Create biased dataset (same as before but cleaner)\n",
    "    np.random.seed(42)\n",
    "    n_samples = 2000\n",
    "    \n",
    "    # Protected attribute: gender\n",
    "    gender = np.random.choice([0, 1], n_samples, p=[0.6, 0.4])  # 0=male, 1=female\n",
    "    \n",
    "    # Create features with bias\n",
    "    experience = np.random.normal(5, 2, n_samples)\n",
    "    education = np.random.normal(7, 1.5, n_samples)\n",
    "    \n",
    "    # Network score biased against women\n",
    "    network_score = np.where(gender == 0,\n",
    "                            np.random.normal(6, 1.5, n_samples),  # Men\n",
    "                            np.random.normal(5, 1.5, n_samples))  # Women\n",
    "    \n",
    "    X = np.column_stack([experience, education, network_score])\n",
    "    \n",
    "    # Create biased hiring decisions\n",
    "    hiring_score = (0.4 * experience + 0.4 * education + 0.2 * network_score - \n",
    "                   0.3 * gender + np.random.normal(0, 1, n_samples))\n",
    "    y = hiring_score > 4.0\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, gender_train, gender_test = train_test_split(\n",
    "        X, y, gender, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Baseline biased model\n",
    "    baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    baseline_model.fit(X_train, y_train)\n",
    "    baseline_pred = baseline_model.predict(X_test)\n",
    "    \n",
    "    # Calculate baseline fairness\n",
    "    male_mask = gender_test == 0\n",
    "    female_mask = gender_test == 1\n",
    "    baseline_male_rate = np.mean(baseline_pred[male_mask])\n",
    "    baseline_female_rate = np.mean(baseline_pred[female_mask])\n",
    "    baseline_dp_diff = baseline_male_rate - baseline_female_rate\n",
    "    \n",
    "    results['baseline'] = {\n",
    "        'accuracy': accuracy_score(y_test, baseline_pred),\n",
    "        'male_positive_rate': baseline_male_rate,\n",
    "        'female_positive_rate': baseline_female_rate,\n",
    "        'demographic_parity_diff': baseline_dp_diff\n",
    "    }\n",
    "    \n",
    "    # TODO: 2. Data rebalancing mitigation\n",
    "    # Balance the training data by gender\n",
    "    male_indices = np.where(gender_train == 0)[0]\n",
    "    female_indices = np.where(gender_train == 1)[0]\n",
    "    \n",
    "    # Downsample majority group or upsample minority group\n",
    "    min_size = min(len(male_indices), len(female_indices))\n",
    "    balanced_indices = None  # np.concatenate([male_indices[:min_size], female_indices[:min_size]])\n",
    "    \n",
    "    # TODO: Train on balanced data\n",
    "    if balanced_indices is not None:\n",
    "        X_balanced = X_train[balanced_indices]\n",
    "        y_balanced = y_train[balanced_indices]\n",
    "        \n",
    "        balanced_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        balanced_model.fit(X_balanced, y_balanced)\n",
    "        balanced_pred = balanced_model.predict(X_test)\n",
    "        \n",
    "        balanced_male_rate = np.mean(balanced_pred[male_mask])\n",
    "        balanced_female_rate = np.mean(balanced_pred[female_mask])\n",
    "        balanced_dp_diff = balanced_male_rate - balanced_female_rate\n",
    "        \n",
    "        results['data_rebalancing'] = {\n",
    "            'accuracy': accuracy_score(y_test, balanced_pred),\n",
    "            'male_positive_rate': balanced_male_rate,\n",
    "            'female_positive_rate': balanced_female_rate,\n",
    "            'demographic_parity_diff': balanced_dp_diff\n",
    "        }\n",
    "    \n",
    "    # TODO: 3. Threshold adjustment mitigation  \n",
    "    # Use different decision thresholds for different groups\n",
    "    baseline_proba = baseline_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Find thresholds that equalize positive rates\n",
    "    male_proba = baseline_proba[male_mask]\n",
    "    female_proba = baseline_proba[female_mask]\n",
    "    \n",
    "    # TODO: Find optimal thresholds\n",
    "    # For simplicity, use fixed thresholds that balance rates\n",
    "    male_threshold = None   # np.percentile(male_proba, 80)  # Raise threshold for advantaged group\n",
    "    female_threshold = None # np.percentile(female_proba, 70) # Lower threshold for disadvantaged group\n",
    "    \n",
    "    if male_threshold is not None and female_threshold is not None:\n",
    "        threshold_pred = np.zeros_like(baseline_pred)\n",
    "        threshold_pred[male_mask] = male_proba >= male_threshold\n",
    "        threshold_pred[female_mask] = female_proba >= female_threshold\n",
    "        \n",
    "        threshold_male_rate = np.mean(threshold_pred[male_mask])\n",
    "        threshold_female_rate = np.mean(threshold_pred[female_mask])\n",
    "        threshold_dp_diff = threshold_male_rate - threshold_female_rate\n",
    "        \n",
    "        results['threshold_adjustment'] = {\n",
    "            'accuracy': accuracy_score(y_test, threshold_pred),\n",
    "            'male_positive_rate': threshold_male_rate,\n",
    "            'female_positive_rate': threshold_female_rate,\n",
    "            'demographic_parity_diff': threshold_dp_diff,\n",
    "            'male_threshold': male_threshold,\n",
    "            'female_threshold': female_threshold\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "@validator.koan(3, \"Bias Mitigation - Reducing Unfair Discrimination\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = implement_bias_mitigation()\n",
    "    \n",
    "    assert 'baseline' in results, \"Baseline model results missing\"\n",
    "    assert results['baseline']['accuracy'] > 0, \"Baseline accuracy not calculated\"\n",
    "    \n",
    "    print(\"✓ Bias mitigation techniques implemented!\")\n",
    "    \n",
    "    print(f\"\\n  📊 Mitigation Results Comparison:\")\n",
    "    print(f\"  {'Method':<20} {'Accuracy':<10} {'Male Rate':<10} {'Female Rate':<12} {'DP Diff':<8}\")\n",
    "    print(f\"  {'-'*65}\")\n",
    "    \n",
    "    for method, metrics in results.items():\n",
    "        acc = metrics['accuracy']\n",
    "        male_rate = metrics['male_positive_rate']\n",
    "        female_rate = metrics['female_positive_rate']\n",
    "        dp_diff = metrics['demographic_parity_diff']\n",
    "        \n",
    "        print(f\"  {method:<20} {acc:<10.3f} {male_rate:<10.3f} {female_rate:<12.3f} {dp_diff:<+8.3f}\")\n",
    "    \n",
    "    # Analyze trade-offs\n",
    "    baseline_acc = results['baseline']['accuracy']\n",
    "    baseline_bias = abs(results['baseline']['demographic_parity_diff'])\n",
    "    \n",
    "    print(f\"\\n  ⚖️ Fairness vs. Accuracy Trade-offs:\")\n",
    "    \n",
    "    for method, metrics in results.items():\n",
    "        if method == 'baseline':\n",
    "            continue\n",
    "            \n",
    "        acc_change = metrics['accuracy'] - baseline_acc\n",
    "        bias_change = baseline_bias - abs(metrics['demographic_parity_diff'])\n",
    "        \n",
    "        print(f\"\\n    {method.replace('_', ' ').title()}:\")\n",
    "        print(f\"      Accuracy change: {acc_change:+.3f}\")\n",
    "        print(f\"      Bias reduction: {bias_change:+.3f}\")\n",
    "        \n",
    "        if bias_change > 0.05:\n",
    "            print(f\"      ✅ Significant bias reduction\")\n",
    "        elif bias_change > 0:\n",
    "            print(f\"      ⚠️ Modest bias reduction\")\n",
    "        else:\n",
    "            print(f\"      🚨 Bias not reduced\")\n",
    "    \n",
    "    print(f\"\\n  🛠️ Mitigation Technique Summary:\")\n",
    "    print(f\"    • Data Rebalancing: Change training data distribution\")\n",
    "    print(f\"    • Threshold Adjustment: Different decision thresholds per group\")\n",
    "    print(f\"    • Fairness Constraints: Add fairness terms to loss function\")\n",
    "    print(f\"    • Adversarial Training: Train to hide protected attributes\")\n",
    "    \n",
    "    print(f\"\\n  💡 Choosing Mitigation Strategies:\")\n",
    "    print(f\"    • Legal requirements (regulatory compliance)\")\n",
    "    print(f\"    • Business context (cost of false positives/negatives)\")\n",
    "    print(f\"    • Stakeholder values (community input)\")\n",
    "    print(f\"    • Technical constraints (available data, compute)\")\n",
    "    \n",
    "    print(f\"\\n  ⚠️ Important Considerations:\")\n",
    "    print(f\"    • Perfect fairness often impossible across all metrics\")\n",
    "    print(f\"    • Mitigation may reduce accuracy\")\n",
    "    print(f\"    • Regular monitoring essential\")\n",
    "    print(f\"    • Transparency with stakeholders critical\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 15.4: Model Interpretability - Explaining AI Decisions\n",
    "**Objective**: Use SHAP and LIME to explain model predictions and identify bias sources  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Understanding why models make certain decisions is crucial for trust, debugging, and regulatory compliance. Interpretability tools help identify when models rely on inappropriate features.\n",
    "\n",
    "**Key Tools**: SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-Agnostic Explanations) provide different approaches to model explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_interpretability_analysis():\n",
    "    \"\"\"\n",
    "    Use interpretability tools to understand model decisions and detect bias.\n",
    "    \n",
    "    We'll train a model and use both global and local explanations\n",
    "    to understand feature importance and individual predictions.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Interpretability analysis results\n",
    "    \"\"\"\n",
    "    # Create interpretable dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Create meaningful features\n",
    "    age = np.random.normal(35, 10, n_samples)\n",
    "    income = np.random.lognormal(10.5, 0.6, n_samples) \n",
    "    credit_score = np.random.normal(650, 100, n_samples)\n",
    "    debt_ratio = np.random.beta(2, 5, n_samples)  # Debt-to-income ratio\n",
    "    employment_years = np.random.exponential(3, n_samples)\n",
    "    \n",
    "    # Create target: loan approval\n",
    "    # Should depend on financial factors, not age\n",
    "    loan_score = (0.3 * (credit_score - 600) / 100 + \n",
    "                 0.4 * np.log(income / 50000) +\n",
    "                 0.2 * employment_years / 10 - \n",
    "                 0.3 * debt_ratio +\n",
    "                 np.random.normal(0, 0.5, n_samples))\n",
    "    \n",
    "    # But introduce age bias in the model training\n",
    "    loan_score += 0.1 * (age - 30) / 10  # Bias: prefer middle-aged applicants\n",
    "    \n",
    "    y = loan_score > 0.5\n",
    "    \n",
    "    # Create DataFrame for easy interpretation\n",
    "    df = pd.DataFrame({\n",
    "        'age': age,\n",
    "        'income': income,\n",
    "        'credit_score': credit_score, \n",
    "        'debt_ratio': debt_ratio,\n",
    "        'employment_years': employment_years,\n",
    "        'approved': y\n",
    "    })\n",
    "    \n",
    "    X = df.drop('approved', axis=1)\n",
    "    y = df['approved']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # TODO: Train interpretable model\n",
    "    model = None  # RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    accuracy = model.score(X_test, y_test) if model else 0\n",
    "    \n",
    "    # Basic feature importance (built-in)\n",
    "    if model:\n",
    "        feature_importance = dict(zip(X.columns, model.feature_importances_))\n",
    "    else:\n",
    "        feature_importance = {}\n",
    "    \n",
    "    # TODO: SHAP Analysis (if available)\n",
    "    shap_analysis = {}\n",
    "    if SHAP_AVAILABLE and model:\n",
    "        try:\n",
    "            # Create SHAP explainer\n",
    "            explainer = None  # shap.TreeExplainer(model)\n",
    "            # shap_values = explainer.shap_values(X_test.iloc[:100])  # Sample for speed\n",
    "            \n",
    "            # Global feature importance from SHAP\n",
    "            # if isinstance(shap_values, list):\n",
    "            #     shap_values = shap_values[1]  # For binary classification\n",
    "            # \n",
    "            # mean_abs_shap = np.mean(np.abs(shap_values), axis=0)\n",
    "            # shap_importance = dict(zip(X.columns, mean_abs_shap))\n",
    "            \n",
    "            shap_analysis = {\n",
    "                'available': True,\n",
    "                'global_importance': {},  # shap_importance\n",
    "                'explanation_type': 'TreeExplainer'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            shap_analysis = {'available': False, 'error': str(e)}\n",
    "    else:\n",
    "        shap_analysis = {'available': False, 'reason': 'SHAP not installed'}\n",
    "    \n",
    "    # TODO: LIME Analysis (if available)  \n",
    "    lime_analysis = {}\n",
    "    if LIME_AVAILABLE and model:\n",
    "        try:\n",
    "            # Create LIME explainer\n",
    "            explainer = None\n",
    "            # explainer = LimeTabularExplainer(\n",
    "            #     X_train.values,\n",
    "            #     feature_names=X.columns.tolist(),\n",
    "            #     class_names=['Denied', 'Approved'],\n",
    "            #     mode='classification'\n",
    "            # )\n",
    "            \n",
    "            # Explain a few instances\n",
    "            # instance_explanations = []\n",
    "            # for i in range(min(5, len(X_test))):\n",
    "            #     explanation = explainer.explain_instance(\n",
    "            #         X_test.iloc[i].values, model.predict_proba\n",
    "            #     )\n",
    "            #     instance_explanations.append(explanation.as_list())\n",
    "            \n",
    "            lime_analysis = {\n",
    "                'available': True,\n",
    "                'sample_explanations': [],  # instance_explanations\n",
    "                'explanation_type': 'TabularExplainer'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            lime_analysis = {'available': False, 'error': str(e)}\n",
    "    else:\n",
    "        lime_analysis = {'available': False, 'reason': 'LIME not installed'}\n",
    "    \n",
    "    # TODO: Bias detection through feature importance\n",
    "    bias_indicators = {}\n",
    "    if feature_importance:\n",
    "        # Check if age has high importance (potential bias)\n",
    "        age_importance = feature_importance.get('age', 0)\n",
    "        total_importance = sum(feature_importance.values())\n",
    "        age_percentage = (age_importance / total_importance) * 100 if total_importance > 0 else 0\n",
    "        \n",
    "        bias_indicators = {\n",
    "            'age_importance_pct': age_percentage,\n",
    "            'high_age_importance': age_percentage > 15,  # Threshold for concern\n",
    "            'most_important_feature': max(feature_importance, key=feature_importance.get) if feature_importance else None\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'feature_importance': feature_importance,\n",
    "        'shap_analysis': shap_analysis,\n",
    "        'lime_analysis': lime_analysis,\n",
    "        'bias_indicators': bias_indicators,\n",
    "        'dataset_size': len(df)\n",
    "    }\n",
    "\n",
    "@validator.koan(4, \"Model Interpretability - Explaining AI Decisions\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = model_interpretability_analysis()\n",
    "    \n",
    "    assert results['model'] is not None, \"Model not trained\"\n",
    "    assert results['accuracy'] > 0, \"Model accuracy not calculated\"\n",
    "    assert len(results['feature_importance']) > 0, \"Feature importance not calculated\"\n",
    "    assert 0.7 <= results['accuracy'] <= 1.0, f\"Model accuracy should be reasonable, got {results['accuracy']:.3f}\"\n",
    "    \n",
    "    print(\"✓ Model interpretability analysis completed!\")\n",
    "    print(f\"  - Model accuracy: {results['accuracy']:.3f}\")\n",
    "    print(f\"  - Dataset size: {results['dataset_size']}\")\n",
    "    \n",
    "    print(f\"\\n  🎯 Feature Importance Analysis:\")\n",
    "    feature_imp = results['feature_importance']\n",
    "    sorted_features = sorted(feature_imp.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for feature, importance in sorted_features:\n",
    "        print(f\"    {feature}: {importance:.3f} ({importance*100:.1f}%)\")\n",
    "    \n",
    "    # Bias detection\n",
    "    bias_indicators = results['bias_indicators']\n",
    "    if bias_indicators.get('high_age_importance'):\n",
    "        print(f\"\\n  🚨 Potential Age Bias Detected!\")\n",
    "        print(f\"    Age importance: {bias_indicators['age_importance_pct']:.1f}%\")\n",
    "        print(f\"    This may indicate age discrimination in the model\")\n",
    "    else:\n",
    "        print(f\"\\n  ✅ Age feature has reasonable importance\")\n",
    "    \n",
    "    most_important = bias_indicators.get('most_important_feature')\n",
    "    if most_important:\n",
    "        print(f\"  🏆 Most important feature: {most_important}\")\n",
    "    \n",
    "    # SHAP availability\n",
    "    shap_status = results['shap_analysis']\n",
    "    if shap_status['available']:\n",
    "        print(f\"\\n  📊 SHAP Analysis: Available\")\n",
    "        print(f\"    • Provides individual prediction explanations\")\n",
    "        print(f\"    • Shows feature contributions for each decision\")\n",
    "        print(f\"    • Helps identify systematic bias patterns\")\n",
    "    else:\n",
    "        print(f\"\\n  📊 SHAP Analysis: Not available\")\n",
    "        print(f\"    Reason: {shap_status.get('reason', shap_status.get('error', 'Unknown'))}\")\n",
    "        print(f\"    Install with: pip install shap\")\n",
    "    \n",
    "    # LIME availability  \n",
    "    lime_status = results['lime_analysis']\n",
    "    if lime_status['available']:\n",
    "        print(f\"\\n  🔍 LIME Analysis: Available\")\n",
    "        print(f\"    • Provides local explanations for individual predictions\")\n",
    "        print(f\"    • Model-agnostic explanation method\")\n",
    "        print(f\"    • Good for explaining specific decisions to users\")\n",
    "    else:\n",
    "        print(f\"\\n  🔍 LIME Analysis: Not available\")\n",
    "        print(f\"    Reason: {lime_status.get('reason', lime_status.get('error', 'Unknown'))}\")\n",
    "        print(f\"    Install with: pip install lime\")\n",
    "    \n",
    "    print(f\"\\n  💡 Interpretability Best Practices:\")\n",
    "    print(f\"    • Use multiple explanation methods (SHAP + LIME + feature importance)\")\n",
    "    print(f\"    • Focus on business-relevant features\")\n",
    "    print(f\"    • Validate explanations with domain experts\")\n",
    "    print(f\"    • Monitor explanations over time\")\n",
    "    \n",
    "    print(f\"\\n  🧭 Regulatory Requirements:\")\n",
    "    print(f\"    • GDPR: Right to explanation for automated decisions\")\n",
    "    print(f\"    • Fair Credit Reporting Act: Adverse action notices\")\n",
    "    print(f\"    • Equal Credit Opportunity Act: No discrimination\")\n",
    "    print(f\"    • Industry standards: Model governance and documentation\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 15.5: Responsible ML Checklist - Comprehensive Ethical Framework\n",
    "**Objective**: Create a comprehensive responsible ML checklist and governance framework  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "This final koan brings together all ethical considerations into a practical framework for responsible ML development and deployment.\n",
    "\n",
    "**Key Elements**: Data governance, model auditing, stakeholder involvement, ongoing monitoring, and incident response procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def responsible_ml_framework():\n",
    "    \"\"\"\n",
    "    Create and evaluate a comprehensive responsible ML framework.\n",
    "    \n",
    "    This includes:\n",
    "    1. Pre-deployment ethical checklist\n",
    "    2. Ongoing monitoring procedures  \n",
    "    3. Stakeholder engagement process\n",
    "    4. Incident response protocol\n",
    "    5. Documentation requirements\n",
    "    \n",
    "    Returns:\n",
    "        dict: Complete responsible ML framework evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Define pre-deployment checklist\n",
    "    pre_deployment_checklist = {\n",
    "        'data_governance': {\n",
    "            'data_source_documented': None,  # True/False\n",
    "            'collection_method_ethical': None,  # True/False  \n",
    "            'consent_obtained': None,  # True/False\n",
    "            'retention_policy_defined': None,  # True/False\n",
    "            'privacy_protected': None  # True/False\n",
    "        },\n",
    "        'bias_assessment': {\n",
    "            'protected_attributes_identified': None,  # True/False\n",
    "            'historical_bias_analyzed': None,  # True/False\n",
    "            'representation_bias_checked': None,  # True/False\n",
    "            'fairness_metrics_calculated': None,  # True/False\n",
    "            'mitigation_strategies_implemented': None  # True/False\n",
    "        },\n",
    "        'model_interpretability': {\n",
    "            'feature_importance_analyzed': None,  # True/False\n",
    "            'decision_boundaries_understood': None,  # True/False\n",
    "            'explanation_tools_implemented': None,  # True/False\n",
    "            'stakeholder_explanations_prepared': None,  # True/False\n",
    "            'adverse_action_notices_ready': None  # True/False\n",
    "        },\n",
    "        'performance_validation': {\n",
    "            'holdout_testing_completed': None,  # True/False\n",
    "            'subgroup_performance_evaluated': None,  # True/False\n",
    "            'edge_cases_tested': None,  # True/False\n",
    "            'error_analysis_conducted': None,  # True/False\n",
    "            'confidence_intervals_calculated': None  # True/False\n",
    "        },\n",
    "        'legal_compliance': {\n",
    "            'regulatory_requirements_reviewed': None,  # True/False\n",
    "            'legal_team_consulted': None,  # True/False\n",
    "            'industry_standards_followed': None,  # True/False\n",
    "            'audit_trail_established': None,  # True/False\n",
    "            'documentation_complete': None  # True/False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # TODO: Define ongoing monitoring framework\n",
    "    monitoring_framework = {\n",
    "        'data_drift_monitoring': {\n",
    "            'feature_distribution_tracking': None,  # True/False\n",
    "            'statistical_tests_automated': None,  # True/False\n",
    "            'alerts_configured': None,  # True/False\n",
    "            'retraining_triggers_defined': None  # True/False\n",
    "        },\n",
    "        'performance_monitoring': {\n",
    "            'accuracy_tracking': None,  # True/False\n",
    "            'subgroup_performance_monitoring': None,  # True/False\n",
    "            'fairness_metrics_tracked': None,  # True/False\n",
    "            'user_feedback_collected': None,  # True/False\n",
    "            'business_metrics_monitored': None  # True/False\n",
    "        },\n",
    "        'bias_monitoring': {\n",
    "            'demographic_parity_tracked': None,  # True/False\n",
    "            'equal_opportunity_monitored': None,  # True/False\n",
    "            'prediction_parity_assessed': None,  # True/False\n",
    "            'intersectional_bias_checked': None,  # True/False\n",
    "            'complaint_tracking_system': None  # True/False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # TODO: Define stakeholder engagement process\n",
    "    stakeholder_engagement = {\n",
    "        'identification': {\n",
    "            'internal_stakeholders_mapped': None,  # True/False\n",
    "            'external_stakeholders_identified': None,  # True/False\n",
    "            'affected_communities_engaged': None,  # True/False\n",
    "            'subject_matter_experts_involved': None  # True/False\n",
    "        },\n",
    "        'communication': {\n",
    "            'regular_updates_scheduled': None,  # True/False\n",
    "            'feedback_mechanisms_established': None,  # True/False\n",
    "            'transparency_reports_published': None,  # True/False\n",
    "            'public_comment_periods_held': None  # True/False\n",
    "        },\n",
    "        'governance': {\n",
    "            'ethics_committee_established': None,  # True/False\n",
    "            'decision_making_process_defined': None,  # True/False\n",
    "            'appeals_process_created': None,  # True/False\n",
    "            'accountability_measures_implemented': None  # True/False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # TODO: Calculate framework completeness score\n",
    "    def calculate_completeness(framework_dict):\n",
    "        total_items = 0\n",
    "        completed_items = 0\n",
    "        \n",
    "        for category in framework_dict.values():\n",
    "            for item, status in category.items():\n",
    "                total_items += 1\n",
    "                if status is True:\n",
    "                    completed_items += 1\n",
    "                    \n",
    "        return completed_items / total_items if total_items > 0 else 0\n",
    "    \n",
    "    # For demonstration, simulate partial completion\n",
    "    # In real scenario, these would be evaluated based on actual implementation\n",
    "    \n",
    "    # Simulate checklist completion (you would evaluate these based on your actual project)\n",
    "    pre_deployment_score = None  # calculate_completeness(pre_deployment_checklist)\n",
    "    monitoring_score = None      # calculate_completeness(monitoring_framework)\n",
    "    engagement_score = None      # calculate_completeness(stakeholder_engagement)\n",
    "    \n",
    "    # TODO: Generate recommendations based on gaps\n",
    "    recommendations = []\n",
    "    \n",
    "    # Check critical gaps\n",
    "    if pre_deployment_score is not None and pre_deployment_score < 0.8:\n",
    "        recommendations.append(\"Complete pre-deployment ethical assessment\")\n",
    "    \n",
    "    if monitoring_score is not None and monitoring_score < 0.7:\n",
    "        recommendations.append(\"Implement comprehensive monitoring system\")\n",
    "        \n",
    "    if engagement_score is not None and engagement_score < 0.6:\n",
    "        recommendations.append(\"Enhance stakeholder engagement process\")\n",
    "    \n",
    "    # Always include these critical recommendations\n",
    "    critical_recommendations = [\n",
    "        \"Establish regular bias audits\",\n",
    "        \"Create incident response procedures\", \n",
    "        \"Implement user feedback systems\",\n",
    "        \"Develop model governance documentation\",\n",
    "        \"Train team on ethical AI principles\"\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        'pre_deployment_checklist': pre_deployment_checklist,\n",
    "        'monitoring_framework': monitoring_framework,\n",
    "        'stakeholder_engagement': stakeholder_engagement,\n",
    "        'pre_deployment_score': pre_deployment_score or 0.75,  # Example score\n",
    "        'monitoring_score': monitoring_score or 0.65,          # Example score\n",
    "        'engagement_score': engagement_score or 0.55,          # Example score\n",
    "        'overall_readiness': ((pre_deployment_score or 0.75) + \n",
    "                             (monitoring_score or 0.65) + \n",
    "                             (engagement_score or 0.55)) / 3,\n",
    "        'recommendations': recommendations + critical_recommendations[:3],\n",
    "        'framework_complete': True\n",
    "    }\n",
    "\n",
    "@validator.koan(5, \"Responsible ML Checklist - Comprehensive Ethical Framework\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = responsible_ml_framework()\n",
    "    \n",
    "    assert results['pre_deployment_checklist'] is not None, \"Pre-deployment checklist not created\"\n",
    "    assert results['monitoring_framework'] is not None, \"Monitoring framework not created\"  \n",
    "    assert results['stakeholder_engagement'] is not None, \"Stakeholder engagement process not defined\"\n",
    "    assert results['framework_complete'] is True, \"Framework not complete\"\n",
    "    assert len(results['recommendations']) > 0, \"No recommendations provided\"\n",
    "    \n",
    "    print(\"✓ Responsible ML framework created successfully!\")\n",
    "    \n",
    "    print(f\"\\n  📊 Framework Readiness Assessment:\")\n",
    "    print(f\"    Pre-deployment readiness: {results['pre_deployment_score']:.1%}\")\n",
    "    print(f\"    Monitoring readiness: {results['monitoring_score']:.1%}\")\n",
    "    print(f\"    Stakeholder engagement: {results['engagement_score']:.1%}\")\n",
    "    print(f\"    Overall readiness: {results['overall_readiness']:.1%}\")\n",
    "    \n",
    "    # Readiness interpretation\n",
    "    overall_readiness = results['overall_readiness']\n",
    "    if overall_readiness >= 0.8:\n",
    "        print(f\"  ✅ Excellent: Ready for responsible deployment\")\n",
    "    elif overall_readiness >= 0.6:\n",
    "        print(f\"  ⚠️ Good: Address key gaps before deployment\")  \n",
    "    else:\n",
    "        print(f\"  🚨 Needs Work: Significant ethical preparation required\")\n",
    "    \n",
    "    print(f\"\\n  🎯 Priority Recommendations:\")\n",
    "    for i, rec in enumerate(results['recommendations'][:5], 1):\n",
    "        print(f\"    {i}. {rec}\")\n",
    "    \n",
    "    print(f\"\\n  📋 Responsible ML Framework Components:\")\n",
    "    \n",
    "    print(f\"\\n  🔍 Pre-Deployment Checklist:\")\n",
    "    checklist_categories = list(results['pre_deployment_checklist'].keys())\n",
    "    for category in checklist_categories:\n",
    "        print(f\"    • {category.replace('_', ' ').title()}\")\n",
    "    \n",
    "    print(f\"\\n  📡 Ongoing Monitoring:\")\n",
    "    monitoring_categories = list(results['monitoring_framework'].keys())\n",
    "    for category in monitoring_categories:\n",
    "        print(f\"    • {category.replace('_', ' ').title()}\")\n",
    "    \n",
    "    print(f\"\\n  🤝 Stakeholder Engagement:\")\n",
    "    engagement_categories = list(results['stakeholder_engagement'].keys())\n",
    "    for category in engagement_categories:\n",
    "        print(f\"    • {category.replace('_', ' ').title()}\")\n",
    "    \n",
    "    print(f\"\\n  🛡️ Critical Success Factors:\")\n",
    "    print(f\"    • Executive leadership commitment\")\n",
    "    print(f\"    • Cross-functional team collaboration\")\n",
    "    print(f\"    • Continuous learning and adaptation\")\n",
    "    print(f\"    • Transparent communication with stakeholders\")\n",
    "    print(f\"    • Regular third-party audits\")\n",
    "    \n",
    "    print(f\"\\n  📚 Essential Resources:\")\n",
    "    print(f\"    • Partnership on AI Tenets\")\n",
    "    print(f\"    • IEEE Standards for Ethical AI\")\n",
    "    print(f\"    • Algorithmic Accountability Act guidelines\")\n",
    "    print(f\"    • Industry-specific ethical guidelines\")\n",
    "    print(f\"    • Academic research on AI ethics\")\n",
    "    \n",
    "    print(f\"\\n  🚀 Implementation Roadmap:\")\n",
    "    print(f\"    1. Form ethics committee and define governance\")\n",
    "    print(f\"    2. Complete pre-deployment checklist\")\n",
    "    print(f\"    3. Implement monitoring and alerting systems\")\n",
    "    print(f\"    4. Engage with stakeholders and communities\")\n",
    "    print(f\"    5. Deploy with gradual rollout and monitoring\")\n",
    "    print(f\"    6. Conduct regular audits and improvements\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations - Data Science Koans Journey Complete!\n",
    "\n",
    "You have not only mastered the technical aspects of machine learning but also embraced the ethical responsibility that comes with building AI systems that affect real human lives.\n",
    "\n",
    "### Your Ethical AI Mastery\n",
    "- ✅ **Fairness Measurement**: Quantified bias with demographic parity and equal opportunity\n",
    "- ✅ **Bias Detection**: Systematically identified discrimination in data and models  \n",
    "- ✅ **Bias Mitigation**: Implemented techniques to reduce unfair treatment\n",
    "- ✅ **Model Interpretability**: Explained AI decisions using SHAP and LIME\n",
    "- ✅ **Responsible ML Framework**: Built comprehensive ethical governance systems\n",
    "\n",
    "### The Journey You've Completed\n",
    "**15 Notebooks | 130+ Koans | Complete ML Mastery**\n",
    "\n",
    "1. **NumPy Fundamentals** → Array operations and broadcasting\n",
    "2. **Pandas Essentials** → Data manipulation and analysis  \n",
    "3. **Data Exploration** → EDA and data profiling\n",
    "4. **Data Cleaning** → Missing values and quality issues\n",
    "5. **Data Transformation** → Feature engineering basics\n",
    "6. **Feature Engineering** → Advanced feature creation\n",
    "7. **Regression Basics** → Linear models and evaluation\n",
    "8. **Classification Basics** → Decision trees and metrics\n",
    "9. **Model Evaluation** → Cross-validation and selection\n",
    "10. **Clustering** → Unsupervised learning methods\n",
    "11. **Dimensionality Reduction** → PCA and manifold learning\n",
    "12. **Ensemble Methods** → Random forests and boosting\n",
    "13. **Hyperparameter Tuning** → Optimization and search\n",
    "14. **Pipelines** → Production ML workflows\n",
    "15. **Ethics and Bias** → Responsible AI practices\n",
    "\n",
    "### Your Professional Impact\n",
    "🎯 **Technical Excellence**: Master-level skills across the ML pipeline  \n",
    "🛡️ **Ethical Leadership**: Champion for responsible AI development  \n",
    "🚀 **Production Ready**: Build deployable, maintainable ML systems  \n",
    "🧭 **Principled Approach**: Balance performance with fairness and transparency  \n",
    "\n",
    "### Beyond the Koans\n",
    "- **Continue Learning**: AI ethics is an evolving field - stay current\n",
    "- **Mentor Others**: Share your knowledge and ethical mindset\n",
    "- **Lead by Example**: Advocate for responsible AI in your organization  \n",
    "- **Stay Engaged**: Participate in AI ethics communities and discussions\n",
    "\n",
    "### Final Wisdom\n",
    "*\"With great ML power comes great responsibility.\"*\n",
    "\n",
    "You now possess both the technical skills to build powerful AI systems and the ethical framework to ensure they benefit all of humanity. Use this knowledge wisely.\n",
    "\n",
    "**Welcome to the community of Responsible AI Practitioners! 🌟**\n",
    "\n",
    "*The world needs more data scientists who care about fairness, transparency, and human dignity. Thank you for joining this mission.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎊 FINAL CELEBRATION - DATA SCIENCE KOANS COMPLETE! 🎊\n",
    "\n",
    "progress = tracker.get_notebook_progress('15_ethics_and_bias')\n",
    "print(f\"📊 Notebook 15 Progress: {progress}% complete!\")\n",
    "\n",
    "# Calculate overall course completion\n",
    "completed_notebooks = 0\n",
    "total_notebooks = 15\n",
    "\n",
    "print(f\"\\n🏆 DATA SCIENCE KOANS - FINAL REPORT 🏆\")\n",
    "print(f\"=\"*50)\n",
    "\n",
    "for i in range(1, 16):\n",
    "    nb_progress = tracker.get_notebook_progress(f'{i:02d}_*')\n",
    "    status = \"✅ COMPLETE\" if nb_progress == 100 else f\"📊 {nb_progress}%\"\n",
    "    notebook_names = [\n",
    "        \"NumPy Fundamentals\", \"Pandas Essentials\", \"Data Exploration\", \n",
    "        \"Data Cleaning\", \"Data Transformation\", \"Feature Engineering\",\n",
    "        \"Regression Basics\", \"Classification Basics\", \"Model Evaluation\",\n",
    "        \"Clustering\", \"Dimensionality Reduction\", \"Ensemble Methods\",\n",
    "        \"Hyperparameter Tuning\", \"Model Pipelines\", \"Ethics and Bias\"\n",
    "    ]\n",
    "    print(f\"Notebook {i:2d}: {notebook_names[i-1]:<25} {status}\")\n",
    "    if nb_progress == 100:\n",
    "        completed_notebooks += 1\n",
    "\n",
    "overall_completion = (completed_notebooks / total_notebooks) * 100\n",
    "print(f\"\\n🎯 OVERALL COURSE COMPLETION: {overall_completion:.1f}%\")\n",
    "print(f\"📚 Notebooks Completed: {completed_notebooks}/{total_notebooks}\")\n",
    "\n",
    "if overall_completion == 100:\n",
    "    print(f\"\\n\" + \"🎉\" * 50)\n",
    "    print(f\"🌟 PHENOMENAL ACHIEVEMENT! 🌟\")\n",
    "    print(f\"\")\n",
    "    print(f\"You have successfully completed ALL Data Science Koans!\")\n",
    "    print(f\"\")\n",
    "    print(f\"🧠 Technical Mastery: ACHIEVED\")\n",
    "    print(f\"🛡️ Ethical Foundation: ESTABLISHED\") \n",
    "    print(f\"🚀 Production Skills: MASTERED\")\n",
    "    print(f\"🧭 Responsible AI: EMBRACED\")\n",
    "    print(f\"\")\n",
    "    print(f\"You are now a COMPLETE Data Science Practitioner!\")\n",
    "    print(f\"\")\n",
    "    print(f\"🎖️ CONGRATULATIONS, ML MASTER! 🎖️\")\n",
    "    print(f\"🎉\" * 50)\n",
    "elif overall_completion >= 90:\n",
    "    print(f\"\\n🌟 Outstanding! You're almost at the finish line!\")\n",
    "    print(f\"🎯 Complete the remaining koans to earn your ML Master status!\")\n",
    "elif overall_completion >= 75:\n",
    "    print(f\"\\n💪 Excellent progress! You're in the advanced stages!\")\n",
    "else:\n",
    "    print(f\"\\n🚀 Great start! Keep building your ML expertise!\")\n",
    "\n",
    "print(f\"\\n📜 Your Data Science Journey:\")\n",
    "print(f\"   ✨ From arrays to ethics\")\n",
    "print(f\"   📈 From data to insights\") \n",
    "print(f\"   🤖 From algorithms to responsibility\")\n",
    "print(f\"   🌍 From code to positive impact\")\n",
    "\n",
    "print(f\"\\n🧭 Remember: Use your powers for good!\")\n",
    "print(f\"   Build AI that serves humanity 🌟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import numpy as np\n",
    "from koans.core.validator import KoanValidator\n",
    "from koans.core.progress import ProgressTracker\n",
    "validator = KoanValidator('15_ethics_and_bias')\n",
    "tracker = ProgressTracker()\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 15.1: Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def koan_1():\n",
    "    # TODO: Complete this exercise\n",
    "    return True\n",
    "\n",
    "@validator.koan(1, 'Ex1', difficulty='Advanced')\n",
    "def validate():\n",
    "    result = koan_1()\n",
    "    assert result == True\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 15.2: Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def koan_2():\n",
    "    # TODO: Complete this exercise\n",
    "    return True\n",
    "\n",
    "@validator.koan(2, 'Ex2', difficulty='Advanced')\n",
    "def validate():\n",
    "    result = koan_2()\n",
    "    assert result == True\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 15.3: Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def koan_3():\n",
    "    # TODO: Complete this exercise\n",
    "    return True\n",
    "\n",
    "@validator.koan(3, 'Ex3', difficulty='Advanced')\n",
    "def validate():\n",
    "    result = koan_3()\n",
    "    assert result == True\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 15.4: Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def koan_4():\n",
    "    # TODO: Complete this exercise\n",
    "    return True\n",
    "\n",
    "@validator.koan(4, 'Ex4', difficulty='Advanced')\n",
    "def validate():\n",
    "    result = koan_4()\n",
    "    assert result == True\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 15.5: Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def koan_5():\n",
    "    # TODO: Complete this exercise\n",
    "    return True\n",
    "\n",
    "@validator.koan(5, 'Ex5', difficulty='Advanced')\n",
    "def validate():\n",
    "    result = koan_5()\n",
    "    assert result == True\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "Ethics and Bias complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = tracker.get_notebook_progress('15_ethics_and_bias')\n",
    "print(f'Progress: {progress}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
