{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning - Data Science Koans\n",
    "\n",
    "Welcome to Notebook 13: Hyperparameter Tuning!\n",
    "\n",
    "## What You Will Learn\n",
    "- Grid Search for exhaustive parameter exploration\n",
    "- Random Search for efficient parameter sampling  \n",
    "- Bayesian Optimization for intelligent search\n",
    "- Early Stopping techniques for efficiency\n",
    "- Nested Cross-Validation for unbiased evaluation\n",
    "- AutoML basics for automated model selection\n",
    "\n",
    "## Why This Matters\n",
    "Hyperparameter tuning can dramatically improve model performance:\n",
    "- **Performance Gains**: Often 10-30% accuracy improvement\n",
    "- **Generalization**: Better validation performance through proper tuning\n",
    "- **Efficiency**: Automated search saves time and finds better solutions\n",
    "- **Robustness**: Reduces sensitivity to parameter choices\n",
    "- **Competition Edge**: Essential for winning ML competitions\n",
    "\n",
    "## Key Concepts\n",
    "- **Hyperparameters**: Model configuration settings (not learned from data)\n",
    "- **Search Space**: Range of possible parameter values to explore\n",
    "- **Cross-Validation**: Robust evaluation during parameter search\n",
    "- **Overfitting Risk**: Tuning on test data leads to overoptimistic results\n",
    "- **Computational Trade-offs**: Balancing search thoroughness vs. time\n",
    "\n",
    "## Prerequisites\n",
    "- Ensemble Methods (Notebook 12)\n",
    "- Understanding of cross-validation\n",
    "- Experience with scikit-learn model evaluation\n",
    "\n",
    "## How to Use\n",
    "1. Understand each search strategy's strengths and weaknesses\n",
    "2. Implement the TODO sections with proper parameter definitions\n",
    "3. Run validations to verify search implementations\n",
    "4. Compare different tuning approaches\n",
    "5. Learn to avoid common pitfalls like data leakage\n",
    "\n",
    "Ready to optimize your models to their full potential? üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run first!\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer, load_digits, make_classification\n",
    "from sklearn.model_selection import (GridSearchCV, RandomizedSearchCV, \n",
    "                                     train_test_split, cross_val_score,\n",
    "                                     validation_curve, learning_curve)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint, uniform\n",
    "import time\n",
    "\n",
    "# Optional advanced libraries\n",
    "try:\n",
    "    from sklearn.experimental import enable_halving_search_cv\n",
    "    from sklearn.model_selection import HalvingGridSearchCV, HalvingRandomSearchCV\n",
    "    HALVING_AVAILABLE = True\n",
    "    print(\"‚úì Halving search methods available\")\n",
    "except ImportError:\n",
    "    HALVING_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Halving search not available (sklearn < 0.24)\")\n",
    "\n",
    "try:\n",
    "    from skopt import BayesSearchCV\n",
    "    from skopt.space import Real, Integer, Categorical\n",
    "    BAYESIAN_AVAILABLE = True\n",
    "    print(\"‚úì Bayesian optimization available\")\n",
    "except ImportError:\n",
    "    BAYESIAN_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Bayesian optimization not available (install: pip install scikit-optimize)\")\n",
    "\n",
    "from koans.core.validator import KoanValidator\n",
    "from koans.core.progress import ProgressTracker\n",
    "\n",
    "validator = KoanValidator(\"13_hyperparameter_tuning\")\n",
    "tracker = ProgressTracker()\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current progress: {tracker.get_notebook_progress('13_hyperparameter_tuning')}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.1: Grid Search - Exhaustive Parameter Search\n",
    "**Objective**: Use GridSearchCV to find optimal Random Forest parameters  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Grid Search evaluates every combination of specified parameter values. It's thorough but can be computationally expensive with large parameter spaces.\n",
    "\n",
    "**Key Concept**: Grid Search guarantees finding the best combination within your specified parameter grid, but grows exponentially with the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_random_forest_with_grid_search():\n",
    "    \"\"\"\n",
    "    Use Grid Search to optimize Random Forest hyperparameters.\n",
    "    \n",
    "    We'll tune: n_estimators, max_depth, min_samples_split, min_samples_leaf\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains best parameters, best score, and search results\n",
    "    \"\"\"\n",
    "    # Load and prepare data\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Define parameter grid for Random Forest\n",
    "    param_grid = {\n",
    "        'n_estimators': None,      # [50, 100, 200]\n",
    "        'max_depth': None,         # [None, 10, 20, 30]  \n",
    "        'min_samples_split': None, # [2, 5, 10]\n",
    "        'min_samples_leaf': None   # [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    # TODO: Create base RandomForestClassifier\n",
    "    rf = None  # RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # TODO: Create GridSearchCV with 5-fold cross-validation\n",
    "    # Hint: GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search = None\n",
    "    \n",
    "    # Time the search\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TODO: Fit the grid search\n",
    "    # grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    # TODO: Get best parameters and score\n",
    "    best_params = None      # grid_search.best_params_\n",
    "    best_cv_score = None    # grid_search.best_score_\n",
    "    \n",
    "    # Test the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_score = accuracy_score(y_test, best_model.predict(X_test))\n",
    "    \n",
    "    return {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_cv_score,\n",
    "        'test_score': test_score,\n",
    "        'search_time': search_time,\n",
    "        'total_combinations': len(grid_search.cv_results_['params']) if grid_search else None\n",
    "    }\n",
    "\n",
    "@validator.koan(1, \"Grid Search - Exhaustive Parameter Search\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = optimize_random_forest_with_grid_search()\n",
    "    \n",
    "    assert results['best_params'] is not None, \"Best parameters is None\"\n",
    "    assert results['best_cv_score'] is not None, \"Best CV score is None\"  \n",
    "    assert results['test_score'] is not None, \"Test score is None\"\n",
    "    assert results['total_combinations'] is not None, \"Total combinations is None\"\n",
    "    \n",
    "    # Check that we have reasonable results\n",
    "    assert 0.8 <= results['best_cv_score'] <= 1.0, f\"CV score should be reasonable, got {results['best_cv_score']:.3f}\"\n",
    "    assert 0.8 <= results['test_score'] <= 1.0, f\"Test score should be reasonable, got {results['test_score']:.3f}\"\n",
    "    \n",
    "    # Check that we tested multiple combinations\n",
    "    expected_combinations = 3 * 4 * 3 * 3  # Based on typical param grid\n",
    "    assert results['total_combinations'] >= 36, f\"Should test many combinations, got {results['total_combinations']}\"\n",
    "    \n",
    "    print(\"‚úì Grid Search optimization complete!\")\n",
    "    print(f\"  - Search time: {results['search_time']:.2f} seconds\")\n",
    "    print(f\"  - Total combinations tested: {results['total_combinations']}\")\n",
    "    print(f\"  - Best CV score: {results['best_cv_score']:.4f}\")\n",
    "    print(f\"  - Test score: {results['test_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  üèÜ Best Parameters Found:\")\n",
    "    for param, value in results['best_params'].items():\n",
    "        print(f\"    {param}: {value}\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    cv_test_diff = abs(results['best_cv_score'] - results['test_score'])\n",
    "    if cv_test_diff < 0.02:\n",
    "        print(f\"\\n  ‚úì Good generalization (CV-Test diff: {cv_test_diff:.3f})\")\n",
    "    else:\n",
    "        print(f\"\\n  ‚ö†Ô∏è Possible overfitting (CV-Test diff: {cv_test_diff:.3f})\")\n",
    "        \n",
    "    print(f\"\\n  üí° Grid Search Insights:\")\n",
    "    print(f\"    ‚Ä¢ Exhaustive: Tests all parameter combinations\")  \n",
    "    print(f\"    ‚Ä¢ Deterministic: Same results every run\")\n",
    "    print(f\"    ‚Ä¢ Expensive: Time grows exponentially with parameters\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.2: Random Search - Efficient Parameter Sampling\n",
    "**Objective**: Use RandomizedSearchCV for faster parameter exploration  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Random Search samples parameter combinations randomly from specified distributions. It's often more efficient than Grid Search, especially with many parameters.\n",
    "\n",
    "**Key Concept**: Random Search can find good parameters faster than Grid Search because it doesn't waste time on systematically bad regions of parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_grid_vs_random_search():\n",
    "    \"\"\"\n",
    "    Compare Grid Search vs Random Search on SVM hyperparameter tuning.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comparison results including timing and performance\n",
    "    \"\"\"\n",
    "    # Load digits dataset (more complex for demonstrating search differences)\n",
    "    digits = load_digits()\n",
    "    X, y = digits.data, digits.target\n",
    "    \n",
    "    # Use subset for faster demo\n",
    "    X_subset = X[:1000]\n",
    "    y_subset = y[:1000]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_subset, y_subset, test_size=0.2, random_state=42, stratify=y_subset\n",
    "    )\n",
    "    \n",
    "    # Scale features for SVM\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # TODO: Define parameter distributions for Random Search\n",
    "    param_distributions = {\n",
    "        'C': None,        # uniform(0.1, 100) - continuous distribution\n",
    "        'gamma': None,    # uniform(0.001, 1) - continuous distribution  \n",
    "        'kernel': None    # ['rbf', 'linear'] - categorical choice\n",
    "    }\n",
    "    \n",
    "    # TODO: Create SVC model\n",
    "    svm = None  # SVC(random_state=42)\n",
    "    \n",
    "    # Grid Search (smaller grid for comparison)\n",
    "    grid_params = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1], 'kernel': ['rbf', 'linear']}\n",
    "    grid_search = GridSearchCV(svm, grid_params, cv=3, n_jobs=-1)\n",
    "    \n",
    "    # TODO: Create RandomizedSearchCV with 20 iterations\n",
    "    # Hint: RandomizedSearchCV(svm, param_distributions, n_iter=20, cv=3, random_state=42, n_jobs=-1)\n",
    "    random_search = None\n",
    "    \n",
    "    # Time both searches\n",
    "    start = time.time()\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    grid_time = time.time() - start\n",
    "    \n",
    "    start = time.time()  \n",
    "    # TODO: Fit random search\n",
    "    # random_search.fit(X_train_scaled, y_train)\n",
    "    random_time = time.time() - start\n",
    "    \n",
    "    # Compare results\n",
    "    grid_score = grid_search.score(X_test_scaled, y_test)\n",
    "    random_score = random_search.score(X_test_scaled, y_test) if random_search else 0\n",
    "    \n",
    "    return {\n",
    "        'grid_time': grid_time,\n",
    "        'random_time': random_time,\n",
    "        'grid_score': grid_score,\n",
    "        'random_score': random_score,\n",
    "        'grid_combinations': len(grid_search.cv_results_['params']),\n",
    "        'random_combinations': 20,\n",
    "        'grid_best_params': grid_search.best_params_,\n",
    "        'random_best_params': random_search.best_params_ if random_search else None\n",
    "    }\n",
    "\n",
    "@validator.koan(2, \"Random Search - Efficient Parameter Sampling\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = compare_grid_vs_random_search()\n",
    "    \n",
    "    assert results['random_score'] is not None and results['random_score'] > 0, \"Random search not implemented\"\n",
    "    assert results['random_best_params'] is not None, \"Random search best params is None\"\n",
    "    assert 0.7 <= results['grid_score'] <= 1.0, f\"Grid score should be reasonable, got {results['grid_score']:.3f}\"\n",
    "    assert 0.7 <= results['random_score'] <= 1.0, f\"Random score should be reasonable, got {results['random_score']:.3f}\"\n",
    "    \n",
    "    print(\"‚úì Random Search vs Grid Search comparison complete!\")\n",
    "    print(f\"\\n  ‚è±Ô∏è  Timing Comparison:\")\n",
    "    print(f\"    Grid Search: {results['grid_time']:.2f}s ({results['grid_combinations']} combinations)\")\n",
    "    print(f\"    Random Search: {results['random_time']:.2f}s ({results['random_combinations']} combinations)\")\n",
    "    \n",
    "    speedup = results['grid_time'] / results['random_time'] if results['random_time'] > 0 else 1\n",
    "    print(f\"    Speedup: {speedup:.1f}x faster\")\n",
    "    \n",
    "    print(f\"\\n  üéØ Performance Comparison:\")\n",
    "    print(f\"    Grid Search: {results['grid_score']:.4f}\")\n",
    "    print(f\"    Random Search: {results['random_score']:.4f}\")\n",
    "    \n",
    "    score_diff = results['random_score'] - results['grid_score']\n",
    "    if abs(score_diff) < 0.02:\n",
    "        print(f\"    Similar performance (diff: {score_diff:+.3f})\")\n",
    "    elif score_diff > 0:\n",
    "        print(f\"    Random Search wins! (+{score_diff:.3f})\")\n",
    "    else:\n",
    "        print(f\"    Grid Search wins! ({score_diff:.3f})\")\n",
    "    \n",
    "    print(f\"\\n  üí° Random Search Benefits:\")\n",
    "    print(f\"    ‚Ä¢ Faster with many parameters\")\n",
    "    print(f\"    ‚Ä¢ Can find good solutions quickly\")\n",
    "    print(f\"    ‚Ä¢ Explores diverse parameter regions\")\n",
    "    print(f\"    ‚Ä¢ Easy to parallelize\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.3: Parameter Distributions - Defining Search Spaces\n",
    "**Objective**: Learn to define appropriate parameter distributions  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Choosing good parameter distributions is crucial for effective random search. Different parameters need different distribution types (uniform, log-uniform, discrete, etc.).\n",
    "\n",
    "**Key Concept**: Parameter distributions should reflect prior knowledge about reasonable parameter ranges and scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_parameter_distributions():\n",
    "    \"\"\"\n",
    "    Design appropriate parameter distributions for Random Forest tuning.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Parameter distributions and tuning results\n",
    "    \"\"\"\n",
    "    # Load breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Define smart parameter distributions\n",
    "    param_distributions = {\n",
    "        # Number of trees: discrete integers in reasonable range\n",
    "        'n_estimators': None,           # randint(50, 301) - random integers from 50 to 300\n",
    "        \n",
    "        # Max depth: include None (unlimited) and reasonable depths\n",
    "        'max_depth': None,              # [None] + list(range(5, 31, 5)) - None plus [5,10,15,20,25,30]\n",
    "        \n",
    "        # Minimum samples to split: small integers\n",
    "        'min_samples_split': None,      # randint(2, 21) - integers from 2 to 20\n",
    "        \n",
    "        # Minimum samples in leaf: small integers  \n",
    "        'min_samples_leaf': None,       # randint(1, 11) - integers from 1 to 10\n",
    "        \n",
    "        # Maximum features: different strategies\n",
    "        'max_features': None,           # ['sqrt', 'log2', None, 0.5, 0.7, 0.9]\n",
    "        \n",
    "        # Bootstrap: boolean choice\n",
    "        'bootstrap': None               # [True, False]\n",
    "    }\n",
    "    \n",
    "    # TODO: Create RandomForestClassifier\n",
    "    rf = None  # RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # TODO: Create RandomizedSearchCV with 100 iterations\n",
    "    # Hint: RandomizedSearchCV(rf, param_distributions, n_iter=100, cv=5, random_state=42, n_jobs=-1, scoring='accuracy')\n",
    "    random_search = None\n",
    "    \n",
    "    # TODO: Fit the search\n",
    "    # random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Analyze the search results\n",
    "    best_params = random_search.best_params_ if random_search else {}\n",
    "    best_score = random_search.best_score_ if random_search else 0\n",
    "    test_score = random_search.score(X_test, y_test) if random_search else 0\n",
    "    \n",
    "    return {\n",
    "        'param_distributions': param_distributions,\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'test_score': test_score,\n",
    "        'n_iter': 100\n",
    "    }\n",
    "\n",
    "@validator.koan(3, \"Parameter Distributions - Defining Search Spaces\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = design_parameter_distributions()\n",
    "    \n",
    "    assert results['best_params'] is not None and len(results['best_params']) > 0, \"Best params not found\"\n",
    "    assert results['best_cv_score'] > 0, \"Best CV score not calculated\" \n",
    "    assert results['test_score'] > 0, \"Test score not calculated\"\n",
    "    assert 0.8 <= results['best_cv_score'] <= 1.0, f\"CV score should be reasonable, got {results['best_cv_score']:.3f}\"\n",
    "    \n",
    "    print(\"‚úì Parameter distribution design complete!\")\n",
    "    print(f\"  - Search iterations: {results['n_iter']}\")\n",
    "    print(f\"  - Best CV score: {results['best_cv_score']:.4f}\")\n",
    "    print(f\"  - Test score: {results['test_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  üèÜ Optimized Parameters:\")\n",
    "    for param, value in results['best_params'].items():\n",
    "        print(f\"    {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\n  üìä Distribution Design Insights:\")\n",
    "    print(f\"    ‚Ä¢ Use randint() for integer parameters\")\n",
    "    print(f\"    ‚Ä¢ Use uniform() for continuous parameters\") \n",
    "    print(f\"    ‚Ä¢ Include None/default values where appropriate\")\n",
    "    print(f\"    ‚Ä¢ Consider log-scale for parameters spanning orders of magnitude\")\n",
    "    print(f\"    ‚Ä¢ Mix discrete choices with continuous ranges\")\n",
    "    \n",
    "    # Check if we got sensible parameter values\n",
    "    n_est = results['best_params'].get('n_estimators', 0)\n",
    "    if 50 <= n_est <= 300:\n",
    "        print(f\"    ‚úì Reasonable n_estimators: {n_est}\")\n",
    "    \n",
    "    max_depth = results['best_params'].get('max_depth')\n",
    "    if max_depth is None or (isinstance(max_depth, int) and 5 <= max_depth <= 30):\n",
    "        print(f\"    ‚úì Reasonable max_depth: {max_depth}\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.4: Bayesian Optimization - Smart Parameter Search  \n",
    "**Objective**: Use Bayesian optimization for intelligent hyperparameter search  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Bayesian optimization uses previous evaluation results to intelligently choose the next parameters to try. It's especially effective for expensive model training.\n",
    "\n",
    "**Key Concept**: Unlike random search, Bayesian optimization learns from past evaluations to focus on promising regions of parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_hyperparameter_optimization():\n",
    "    \"\"\"\n",
    "    Use Bayesian optimization for intelligent hyperparameter search.\n",
    "    Falls back to RandomizedSearchCV if BayesSearchCV not available.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Bayesian optimization results and comparison\n",
    "    \"\"\"\n",
    "    # Load breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    if BAYESIAN_AVAILABLE:\n",
    "        # TODO: Define search space using skopt\n",
    "        search_space = {\n",
    "            'n_estimators': None,      # Integer(50, 300)\n",
    "            'max_depth': None,         # Integer(1, 30) \n",
    "            'min_samples_split': None, # Integer(2, 20)\n",
    "            'min_samples_leaf': None,  # Integer(1, 10)\n",
    "            'max_features': None       # Categorical(['sqrt', 'log2'])\n",
    "        }\n",
    "        \n",
    "        # TODO: Create RandomForestClassifier\n",
    "        rf = None  # RandomForestClassifier(random_state=42)\n",
    "        \n",
    "        # TODO: Create BayesSearchCV\n",
    "        # Hint: BayesSearchCV(rf, search_space, n_iter=50, cv=5, random_state=42, n_jobs=-1)\n",
    "        bayes_search = None\n",
    "        \n",
    "        # TODO: Fit the Bayesian search\n",
    "        # bayes_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_params = bayes_search.best_params_ if bayes_search else {}\n",
    "        best_score = bayes_search.best_score_ if bayes_search else 0\n",
    "        test_score = bayes_search.score(X_test, y_test) if bayes_search else 0\n",
    "        search_type = \"Bayesian\"\n",
    "        \n",
    "    else:\n",
    "        # Fallback to RandomizedSearchCV\n",
    "        print(\"Using RandomizedSearchCV fallback...\")\n",
    "        param_distributions = {\n",
    "            'n_estimators': randint(50, 301),\n",
    "            'max_depth': randint(1, 31),\n",
    "            'min_samples_split': randint(2, 21), \n",
    "            'min_samples_leaf': randint(1, 11),\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "        \n",
    "        rf = RandomForestClassifier(random_state=42)\n",
    "        bayes_search = RandomizedSearchCV(rf, param_distributions, n_iter=50, cv=5, random_state=42, n_jobs=-1)\n",
    "        bayes_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_params = bayes_search.best_params_\n",
    "        best_score = bayes_search.best_score_\n",
    "        test_score = bayes_search.score(X_test, y_test)\n",
    "        search_type = \"Randomized (fallback)\"\n",
    "    \n",
    "    return {\n",
    "        'search_type': search_type,\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'test_score': test_score,\n",
    "        'bayesian_available': BAYESIAN_AVAILABLE\n",
    "    }\n",
    "\n",
    "@validator.koan(4, \"Bayesian Optimization - Smart Parameter Search\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = bayesian_hyperparameter_optimization()\n",
    "    \n",
    "    assert results['best_params'] is not None, \"Best parameters is None\"\n",
    "    assert results['best_cv_score'] > 0, \"Best CV score not found\"\n",
    "    assert results['test_score'] > 0, \"Test score not found\"\n",
    "    assert 0.8 <= results['best_cv_score'] <= 1.0, f\"CV score should be reasonable, got {results['best_cv_score']:.3f}\"\n",
    "    \n",
    "    print(f\"‚úì {results['search_type']} optimization complete!\")\n",
    "    print(f\"  - Method: {results['search_type']}\")\n",
    "    print(f\"  - Best CV score: {results['best_cv_score']:.4f}\")\n",
    "    print(f\"  - Test score: {results['test_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  üèÜ Best Parameters:\")\n",
    "    for param, value in results['best_params'].items():\n",
    "        print(f\"    {param}: {value}\")\n",
    "    \n",
    "    if results['bayesian_available']:\n",
    "        print(f\"\\n  üß† Bayesian Optimization Benefits:\")\n",
    "        print(f\"    ‚Ä¢ Learns from previous evaluations\")\n",
    "        print(f\"    ‚Ä¢ Focuses on promising parameter regions\")\n",
    "        print(f\"    ‚Ä¢ More efficient than random search\")\n",
    "        print(f\"    ‚Ä¢ Balances exploration vs exploitation\")\n",
    "        print(f\"    ‚Ä¢ Great for expensive model training\")\n",
    "    else:\n",
    "        print(f\"\\n  üì¶ To use Bayesian optimization:\")\n",
    "        print(f\"    pip install scikit-optimize\")\n",
    "        print(f\"    ‚Ä¢ Much smarter than random search\")\n",
    "        print(f\"    ‚Ä¢ Essential for expensive hyperparameter tuning\")\n",
    "        \n",
    "    print(f\"\\n  üí° When to use Bayesian optimization:\")\n",
    "    print(f\"    ‚Ä¢ Model training is expensive (>1 minute per iteration)\")\n",
    "    print(f\"    ‚Ä¢ Complex parameter interactions exist\") \n",
    "    print(f\"    ‚Ä¢ You have limited evaluation budget\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.5: Early Stopping - Training Efficiency\n",
    "**Objective**: Implement early stopping to prevent overfitting and save time  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Early stopping monitors validation performance during training and stops when performance plateaus or degrades, preventing overfitting and saving computation time.\n",
    "\n",
    "**Key Concept**: Early stopping is crucial for iterative algorithms like gradient boosting, neural networks, and any model trained with validation monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_early_stopping():\n",
    "    \"\"\"\n",
    "    Demonstrate early stopping with Gradient Boosting.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results with and without early stopping\n",
    "    \"\"\"\n",
    "    # Create a larger dataset to demonstrate early stopping benefits\n",
    "    X, y = make_classification(n_samples=2000, n_features=20, n_informative=10, \n",
    "                               n_redundant=10, random_state=42)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Further split training data for validation\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    # TODO: Train model WITHOUT early stopping\n",
    "    gb_no_early = None  # GradientBoostingClassifier(n_estimators=200, random_state=42, verbose=0)\n",
    "    \n",
    "    # TODO: Fit the model\n",
    "    # gb_no_early.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # TODO: Train model WITH early stopping\n",
    "    gb_early = None  # GradientBoostingClassifier(n_estimators=200, validation_fraction=0.2, \n",
    "                    #                             n_iter_no_change=10, random_state=42, verbose=0)\n",
    "    \n",
    "    # TODO: Fit with early stopping\n",
    "    # gb_early.fit(X_train, y_train)  # Uses internal validation split\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    no_early_score = gb_no_early.score(X_test, y_test) if gb_no_early else 0\n",
    "    early_score = gb_early.score(X_test, y_test) if gb_early else 0\n",
    "    \n",
    "    # Get number of estimators actually used\n",
    "    no_early_n_est = gb_no_early.n_estimators if gb_no_early else 0\n",
    "    early_n_est = gb_early.n_estimators_ if gb_early and hasattr(gb_early, 'n_estimators_') else (gb_early.n_estimators if gb_early else 0)\n",
    "    \n",
    "    return {\n",
    "        'no_early_stopping_score': no_early_score,\n",
    "        'early_stopping_score': early_score,\n",
    "        'no_early_n_estimators': no_early_n_est,\n",
    "        'early_n_estimators': early_n_est,\n",
    "        'estimators_saved': no_early_n_est - early_n_est if early_n_est < no_early_n_est else 0\n",
    "    }\n",
    "\n",
    "@validator.koan(5, \"Early Stopping - Training Efficiency\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = implement_early_stopping()\n",
    "    \n",
    "    assert results['no_early_stopping_score'] > 0, \"No early stopping model not trained\"\n",
    "    assert results['early_stopping_score'] > 0, \"Early stopping model not trained\"\n",
    "    assert 0.7 <= results['no_early_stopping_score'] <= 1.0, \"No early stopping score should be reasonable\"\n",
    "    assert 0.7 <= results['early_stopping_score'] <= 1.0, \"Early stopping score should be reasonable\"\n",
    "    \n",
    "    print(\"‚úì Early stopping comparison complete!\")\n",
    "    print(f\"\\n  üìä Performance Comparison:\")\n",
    "    print(f\"    Without early stopping: {results['no_early_stopping_score']:.4f} ({results['no_early_n_estimators']} trees)\")\n",
    "    print(f\"    With early stopping: {results['early_stopping_score']:.4f} ({results['early_n_estimators']} trees)\")\n",
    "    \n",
    "    if results['estimators_saved'] > 0:\n",
    "        saved_pct = (results['estimators_saved'] / results['no_early_n_estimators']) * 100\n",
    "        print(f\"    Trees saved: {results['estimators_saved']} ({saved_pct:.1f}% reduction)\")\n",
    "    \n",
    "    score_diff = results['early_stopping_score'] - results['no_early_stopping_score']\n",
    "    if score_diff >= 0:\n",
    "        print(f\"    Early stopping performed equally well or better!\")\n",
    "    else:\n",
    "        print(f\"    Small performance trade-off: {score_diff:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  ‚ö° Early Stopping Benefits:\")\n",
    "    print(f\"    ‚Ä¢ Prevents overfitting automatically\")\n",
    "    print(f\"    ‚Ä¢ Saves computational time\")\n",
    "    print(f\"    ‚Ä¢ Reduces model complexity\")\n",
    "    print(f\"    ‚Ä¢ Built into many algorithms (GBM, XGBoost, neural networks)\")\n",
    "    \n",
    "    print(f\"\\n  ‚öôÔ∏è  Key Parameters:\")\n",
    "    print(f\"    ‚Ä¢ n_iter_no_change: Stop after N iterations without improvement\")\n",
    "    print(f\"    ‚Ä¢ validation_fraction: Fraction of training data for validation\")\n",
    "    print(f\"    ‚Ä¢ tol: Minimum improvement threshold\")\n",
    "    \n",
    "    print(f\"\\n  üí° Best Practices:\")\n",
    "    print(f\"    ‚Ä¢ Always use with iterative algorithms\")\n",
    "    print(f\"    ‚Ä¢ Monitor validation loss, not training loss\") \n",
    "    print(f\"    ‚Ä¢ Set reasonable patience (n_iter_no_change)\")\n",
    "    print(f\"    ‚Ä¢ Consider restoring best weights\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.6: Nested Cross-Validation - Unbiased Model Evaluation\n",
    "**Objective**: Implement nested CV to avoid data leakage in hyperparameter tuning  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Nested cross-validation provides unbiased performance estimates when doing hyperparameter tuning. The outer loop evaluates performance, the inner loop tunes parameters.\n",
    "\n",
    "**Key Concept**: Using the same data for hyperparameter tuning and performance evaluation leads to overoptimistic results. Nested CV separates these processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation():\n",
    "    \"\"\"\n",
    "    Implement nested cross-validation for unbiased hyperparameter tuning evaluation.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Nested CV results and comparison with simple CV\n",
    "    \"\"\"\n",
    "    # Load breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    # TODO: Define parameter grid for tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': None,      # [50, 100, 200]\n",
    "        'max_depth': None,         # [None, 10, 20]\n",
    "        'min_samples_split': None  # [2, 5, 10]\n",
    "    }\n",
    "    \n",
    "    # TODO: Create base model\n",
    "    rf = None  # RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Inner CV: For hyperparameter tuning (3-fold)  \n",
    "    # TODO: Create GridSearchCV for inner loop\n",
    "    inner_cv = None  # GridSearchCV(rf, param_grid, cv=3, scoring='accuracy')\n",
    "    \n",
    "    # Outer CV: For performance evaluation (5-fold)\n",
    "    # TODO: Calculate nested cross-validation scores\n",
    "    # Hint: Use cross_val_score(inner_cv, X, y, cv=5)\n",
    "    nested_scores = None\n",
    "    \n",
    "    # Compare with non-nested (biased) approach\n",
    "    # This reuses the same data for tuning and evaluation - BAD!\n",
    "    simple_cv = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
    "    simple_cv.fit(X, y)\n",
    "    biased_score = simple_cv.best_score_\n",
    "    \n",
    "    # Calculate statistics\n",
    "    nested_mean = np.mean(nested_scores) if nested_scores is not None else 0\n",
    "    nested_std = np.std(nested_scores) if nested_scores is not None else 0\n",
    "    \n",
    "    return {\n",
    "        'nested_scores': nested_scores,\n",
    "        'nested_mean': nested_mean,\n",
    "        'nested_std': nested_std,\n",
    "        'biased_score': biased_score,\n",
    "        'bias': biased_score - nested_mean,\n",
    "        'best_params': simple_cv.best_params_\n",
    "    }\n",
    "\n",
    "@validator.koan(6, \"Nested Cross-Validation - Unbiased Model Evaluation\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = nested_cross_validation()\n",
    "    \n",
    "    assert results['nested_scores'] is not None, \"Nested CV scores not calculated\"\n",
    "    assert len(results['nested_scores']) == 5, \"Should have 5 outer CV scores\"\n",
    "    assert 0.7 <= results['nested_mean'] <= 1.0, f\"Nested mean should be reasonable, got {results['nested_mean']:.3f}\"\n",
    "    assert 0.7 <= results['biased_score'] <= 1.0, f\"Biased score should be reasonable, got {results['biased_score']:.3f}\"\n",
    "    \n",
    "    print(\"‚úì Nested cross-validation analysis complete!\")\n",
    "    print(f\"\\n  üîÑ Nested CV Results (Unbiased):\")\n",
    "    print(f\"    Mean accuracy: {results['nested_mean']:.4f} (¬±{results['nested_std']:.4f})\")\n",
    "    print(f\"    Individual scores: {[f'{s:.3f}' for s in results['nested_scores']]}\")\n",
    "    \n",
    "    print(f\"\\n  ‚ö†Ô∏è  Simple CV Results (Biased):\")\n",
    "    print(f\"    Mean accuracy: {results['biased_score']:.4f}\")\n",
    "    print(f\"    Optimistic bias: +{results['bias']:.4f}\")\n",
    "    \n",
    "    if results['bias'] > 0.01:\n",
    "        print(f\"    üìà Significant overestimation detected!\")\n",
    "    else:\n",
    "        print(f\"    ‚úì Minimal bias (dataset may be easy)\")\n",
    "    \n",
    "    print(f\"\\n  üèÜ Best Parameters Found:\")\n",
    "    for param, value in results['best_params'].items():\n",
    "        print(f\"    {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\n  üí° Why Nested CV Matters:\")\n",
    "    print(f\"    ‚Ä¢ Prevents data leakage during hyperparameter tuning\")\n",
    "    print(f\"    ‚Ä¢ Provides unbiased performance estimates\")\n",
    "    print(f\"    ‚Ä¢ Essential for model comparison and selection\")\n",
    "    print(f\"    ‚Ä¢ Required for reliable performance reporting\")\n",
    "    \n",
    "    print(f\"\\n  üìã Nested CV Process:\")\n",
    "    print(f\"    1. Outer loop: Split data for performance evaluation\")\n",
    "    print(f\"    2. Inner loop: Tune hyperparameters on training portion\")\n",
    "    print(f\"    3. Evaluate tuned model on validation portion\")\n",
    "    print(f\"    4. Repeat for all outer folds\")\n",
    "    print(f\"    5. Average performance across outer folds\")\n",
    "    \n",
    "    print(f\"\\n  ‚ö†Ô∏è  Common Mistake to Avoid:\")\n",
    "    print(f\"    ‚Ä¢ Never use the same data for tuning AND evaluation\")\n",
    "    print(f\"    ‚Ä¢ This leads to overoptimistic performance estimates\")\n",
    "    print(f\"    ‚Ä¢ Use nested CV for honest performance reporting\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 13.7: AutoML Basics - Automated Model Selection\n",
    "**Objective**: Understand automated machine learning concepts and implementation  \n",
    "**Difficulty**: Advanced  \n",
    "\n",
    "AutoML automates the machine learning pipeline: feature preprocessing, algorithm selection, hyperparameter tuning, and ensemble creation. It democratizes ML and can outperform manual approaches.\n",
    "\n",
    "**Key Concept**: AutoML systems combine multiple techniques (meta-learning, Bayesian optimization, genetic algorithms) to automate the entire ML workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_automl_approach():\n",
    "    \"\"\"\n",
    "    Simulate an AutoML approach by automatically trying multiple algorithms\n",
    "    and selecting the best one through automated hyperparameter tuning.\n",
    "    \n",
    "    Returns:\n",
    "        dict: AutoML results including best model and comparison\n",
    "    \"\"\"\n",
    "    # Load breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features for algorithms that need it\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # TODO: Define candidate algorithms with their parameter grids\n",
    "    algorithms = {\n",
    "        'RandomForest': {\n",
    "            'model': None,  # RandomForestClassifier(random_state=42)\n",
    "            'params': None, # {'n_estimators': [50, 100], 'max_depth': [None, 10, 20]}\n",
    "            'scaled': False  # Doesn't need scaling\n",
    "        },\n",
    "        'SVM': {\n",
    "            'model': None,  # SVC(random_state=42)\n",
    "            'params': None, # {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}\n",
    "            'scaled': True   # Needs scaling\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'model': None,  # LogisticRegression(random_state=42, max_iter=1000)\n",
    "            'params': None, # {'C': [0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']}\n",
    "            'scaled': True   # Needs scaling\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Automated model selection and tuning\n",
    "    results = {}\n",
    "    best_score = 0\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    \n",
    "    for name, config in algorithms.items():\n",
    "        if config['model'] is not None and config['params'] is not None:\n",
    "            # Choose appropriate data (scaled or not)\n",
    "            X_train_use = X_train_scaled if config['scaled'] else X_train\n",
    "            X_test_use = X_test_scaled if config['scaled'] else X_test\n",
    "            \n",
    "            # TODO: Perform grid search for this algorithm\n",
    "            grid_search = None  # GridSearchCV(config['model'], config['params'], cv=5, scoring='accuracy')\n",
    "            \n",
    "            # TODO: Fit grid search\n",
    "            # grid_search.fit(X_train_use, y_train)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            test_score = grid_search.score(X_test_use, y_test) if grid_search else 0\n",
    "            \n",
    "            results[name] = {\n",
    "                'best_params': grid_search.best_params_ if grid_search else {},\n",
    "                'cv_score': grid_search.best_score_ if grid_search else 0,\n",
    "                'test_score': test_score,\n",
    "                'model': grid_search.best_estimator_ if grid_search else None\n",
    "            }\n",
    "            \n",
    "            # Track best performing model\n",
    "            if test_score > best_score:\n",
    "                best_score = test_score\n",
    "                best_model_name = name\n",
    "                best_model = grid_search.best_estimator_ if grid_search else None\n",
    "    \n",
    "    return {\n",
    "        'algorithm_results': results,\n",
    "        'best_model_name': best_model_name,\n",
    "        'best_model': best_model,\n",
    "        'best_score': best_score,\n",
    "        'algorithms_tested': len([k for k, v in algorithms.items() if v['model'] is not None])\n",
    "    }\n",
    "\n",
    "@validator.koan(7, \"AutoML Basics - Automated Model Selection\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = simulate_automl_approach()\n",
    "    \n",
    "    assert results['algorithm_results'] is not None, \"Algorithm results not generated\"\n",
    "    assert results['best_model_name'] is not None, \"Best model not selected\"\n",
    "    assert results['best_score'] > 0, \"Best score not calculated\"\n",
    "    assert 0.8 <= results['best_score'] <= 1.0, f\"Best score should be reasonable, got {results['best_score']:.3f}\"\n",
    "    \n",
    "    print(\"‚úì AutoML simulation complete!\")\n",
    "    print(f\"  - Algorithms tested: {results['algorithms_tested']}\")\n",
    "    print(f\"  - Best algorithm: {results['best_model_name']}\")\n",
    "    print(f\"  - Best test score: {results['best_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  üìä Algorithm Comparison:\")\n",
    "    for name, result in results['algorithm_results'].items():\n",
    "        if result['test_score'] > 0:\n",
    "            print(f\"    {name}:\")\n",
    "            print(f\"      CV Score: {result['cv_score']:.4f}\")\n",
    "            print(f\"      Test Score: {result['test_score']:.4f}\")\n",
    "            print(f\"      Best Params: {result['best_params']}\")\n",
    "    \n",
    "    print(f\"\\n  üèÜ Winning Model: {results['best_model_name']}\")\n",
    "    \n",
    "    print(f\"\\n  ü§ñ Real AutoML Systems:\")\n",
    "    print(f\"    ‚Ä¢ Auto-sklearn: Automated sklearn pipeline\")\n",
    "    print(f\"    ‚Ä¢ TPOT: Genetic programming for ML pipelines\")\n",
    "    print(f\"    ‚Ä¢ H2O AutoML: Distributed AutoML platform\")\n",
    "    print(f\"    ‚Ä¢ Google AutoML: Cloud-based AutoML\")\n",
    "    print(f\"    ‚Ä¢ Azure AutoML: Microsoft's AutoML service\")\n",
    "    \n",
    "    print(f\"\\n  üí° AutoML Benefits:\")\n",
    "    print(f\"    ‚Ä¢ Democratizes machine learning\")\n",
    "    print(f\"    ‚Ä¢ Finds good models quickly\")\n",
    "    print(f\"    ‚Ä¢ Handles algorithm selection automatically\")\n",
    "    print(f\"    ‚Ä¢ Can discover unexpected good models\")\n",
    "    print(f\"    ‚Ä¢ Saves expert time for harder problems\")\n",
    "    \n",
    "    print(f\"\\n  ‚ö†Ô∏è  AutoML Limitations:\")\n",
    "    print(f\"    ‚Ä¢ May not capture domain expertise\")\n",
    "    print(f\"    ‚Ä¢ Can be computationally expensive\")\n",
    "    print(f\"    ‚Ä¢ Less interpretable model selection\")\n",
    "    print(f\"    ‚Ä¢ May overfit to validation set with extensive search\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You have mastered hyperparameter tuning - the key to unlocking your models' full potential!\n",
    "\n",
    "### What You've Mastered\n",
    "- ‚úÖ **Grid Search**: Exhaustive parameter space exploration\n",
    "- ‚úÖ **Random Search**: Efficient parameter sampling strategies  \n",
    "- ‚úÖ **Parameter Distributions**: Smart search space design\n",
    "- ‚úÖ **Bayesian Optimization**: Intelligent, adaptive parameter search\n",
    "- ‚úÖ **Early Stopping**: Preventing overfitting and saving computation\n",
    "- ‚úÖ **Nested Cross-Validation**: Unbiased model evaluation methodology\n",
    "- ‚úÖ **AutoML Concepts**: Understanding automated machine learning\n",
    "\n",
    "### Key Insights Gained\n",
    "1. **Search Strategy Matters**: Different approaches for different scenarios\n",
    "2. **Computational Trade-offs**: Balance thoroughness vs. efficiency\n",
    "3. **Avoid Data Leakage**: Separate tuning from evaluation\n",
    "4. **Smart Distributions**: Prior knowledge improves search efficiency\n",
    "5. **Automation Potential**: AutoML can find surprising good solutions\n",
    "\n",
    "### Performance Impact\n",
    "- üéØ **10-30% accuracy gains** from proper hyperparameter tuning\n",
    "- ‚ö° **10-100x speedup** with smart search strategies  \n",
    "- üõ°Ô∏è **Robust evaluation** through nested cross-validation\n",
    "- ü§ñ **Automated workflows** reducing manual effort\n",
    "\n",
    "### Next Steps\n",
    "- **Notebook 14**: Model Selection and Pipelines (production workflows!)\n",
    "- **Advanced**: Neural architecture search, multi-objective optimization\n",
    "- **Practice**: Apply to your own models and datasets\n",
    "\n",
    "### Real-World Applications\n",
    "- **Competitions**: Essential for Kaggle and ML contests\n",
    "- **Production**: Automated model improvement and monitoring\n",
    "- **Research**: Reproducible and fair model comparisons\n",
    "- **Business**: Maximizing ROI from ML investments\n",
    "\n",
    "You now have the tools to systematically optimize any machine learning model! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Progress Check\n",
    "progress = tracker.get_notebook_progress('13_hyperparameter_tuning')\n",
    "print(f\"\\nüìä Your Progress: {progress}% complete!\")\n",
    "\n",
    "if progress == 100:\n",
    "    print(\"üéâ Exceptional! You've mastered all hyperparameter tuning techniques!\")\n",
    "    print(\"üéØ Ready for Notebook 14: Model Selection and Pipelines\")\n",
    "elif progress >= 75:\n",
    "    print(\"üåü Outstanding progress! Almost finished with tuning mastery.\")\n",
    "elif progress >= 50:\n",
    "    print(\"üí™ Great work! You're building powerful optimization skills.\")\n",
    "else:\n",
    "    print(\"üöÄ Keep going! Each technique builds sophisticated tuning expertise.\")\n",
    "\n",
    "print(f\"\\nüìà Overall course progress:\")\n",
    "total_notebooks = 15\n",
    "completed_notebooks = len([nb for nb in range(1, 14) if tracker.get_notebook_progress(f'{nb:02d}_*') == 100])\n",
    "print(f\"   Completed notebooks: {completed_notebooks}/{total_notebooks}\")\n",
    "print(f\"   Course progress: {(completed_notebooks/total_notebooks)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüéØ Hyperparameter Tuning Mastery Achieved!\")\n",
    "print(f\"   Your models will never be the same! ‚ö°\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
