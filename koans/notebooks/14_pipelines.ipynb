{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Pipelines - Data Science Koans\n",
    "\n",
    "Welcome to Notebook 14: Model Selection and Pipelines!\n",
    "\n",
    "## What You Will Learn\n",
    "- Pipeline fundamentals for workflow automation\n",
    "- ColumnTransformer for feature-specific preprocessing  \n",
    "- Custom transformers extending scikit-learn\n",
    "- Integrated hyperparameter tuning with pipelines\n",
    "- Production-ready ML pipelines\n",
    "\n",
    "## Why This Matters\n",
    "ML Pipelines are essential for production machine learning because they:\n",
    "- **Prevent Data Leakage**: Ensure proper train/test separation\n",
    "- **Ensure Reproducibility**: Same preprocessing steps every time\n",
    "- **Enable Automation**: From raw data to predictions in one step\n",
    "- **Simplify Deployment**: Encapsulate entire workflow in one object\n",
    "- **Reduce Errors**: Eliminate manual preprocessing mistakes\n",
    "\n",
    "## Key Concepts\n",
    "- **Pipeline**: Sequential chain of data transformations + final estimator\n",
    "- **ColumnTransformer**: Apply different transformations to different columns\n",
    "- **Custom Transformers**: Extend sklearn with domain-specific preprocessing\n",
    "- **Pipeline + GridSearch**: Tune preprocessing AND model parameters together\n",
    "- **Production Considerations**: Serialization, versioning, monitoring\n",
    "\n",
    "## Prerequisites  \n",
    "- Hyperparameter Tuning (Notebook 13)\n",
    "- Understanding of preprocessing techniques\n",
    "- Experience with scikit-learn workflows\n",
    "\n",
    "## How to Use\n",
    "1. Build increasingly sophisticated pipeline components\n",
    "2. Learn to avoid common data leakage pitfalls\n",
    "3. Implement custom preprocessing logic\n",
    "4. Combine pipelines with hyperparameter tuning\n",
    "5. Create deployment-ready ML workflows\n",
    "\n",
    "Ready to build production-grade ML pipelines? Let's automate your workflow! 🔧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run first!\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer, load_wine, make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib\n",
    "\n",
    "from koans.core.validator import KoanValidator\n",
    "from koans.core.progress import ProgressTracker\n",
    "\n",
    "validator = KoanValidator(\"14_pipelines\")\n",
    "tracker = ProgressTracker()\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current progress: {tracker.get_notebook_progress('14_pipelines')}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 14.1: Pipeline Basics - Workflow Automation\n",
    "**Objective**: Build a basic pipeline chaining preprocessing and modeling  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Pipelines ensure that preprocessing steps are applied consistently during training and prediction, preventing data leakage and ensuring reproducible results.\n",
    "\n",
    "**Key Concept**: Pipelines apply transformations in sequence, with the final step being an estimator. Each step gets the output of the previous step as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_pipeline():\n",
    "    \"\"\"\n",
    "    Create a basic pipeline that scales features and applies logistic regression.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Pipeline performance results\n",
    "    \"\"\"\n",
    "    # Load breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Create a pipeline with StandardScaler and LogisticRegression\n",
    "    # Pipeline steps should be: [('scaler', StandardScaler()), ('classifier', LogisticRegression(random_state=42))]\n",
    "    pipeline = None\n",
    "    \n",
    "    # TODO: Fit the pipeline\n",
    "    # pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # TODO: Make predictions\n",
    "    # y_pred = pipeline.predict(X_test)\n",
    "    # accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy = None\n",
    "    \n",
    "    # Test individual steps\n",
    "    scaler_step = pipeline.named_steps['scaler'] if pipeline else None\n",
    "    classifier_step = pipeline.named_steps['classifier'] if pipeline else None\n",
    "    \n",
    "    return {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'scaler': scaler_step,\n",
    "        'classifier': classifier_step,\n",
    "        'pipeline_steps': len(pipeline.steps) if pipeline else 0\n",
    "    }\n",
    "\n",
    "@validator.koan(1, \"Pipeline Basics - Workflow Automation\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = create_basic_pipeline()\n",
    "    \n",
    "    assert results['pipeline'] is not None, \"Pipeline not created\"\n",
    "    assert results['accuracy'] is not None, \"Accuracy not calculated\"\n",
    "    assert results['pipeline_steps'] == 2, f\"Pipeline should have 2 steps, got {results['pipeline_steps']}\"\n",
    "    assert 0.85 <= results['accuracy'] <= 1.0, f\"Accuracy should be reasonable, got {results['accuracy']:.3f}\"\n",
    "    \n",
    "    # Check pipeline components\n",
    "    assert results['scaler'] is not None, \"Scaler step not found\"\n",
    "    assert results['classifier'] is not None, \"Classifier step not found\"\n",
    "    assert isinstance(results['scaler'], StandardScaler), \"First step should be StandardScaler\"\n",
    "    assert isinstance(results['classifier'], LogisticRegression), \"Second step should be LogisticRegression\"\n",
    "    \n",
    "    print(\"✓ Basic pipeline created successfully!\")\n",
    "    print(f\"  - Pipeline steps: {results['pipeline_steps']}\")\n",
    "    print(f\"  - Test accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"  - Scaler: {type(results['scaler']).__name__}\")\n",
    "    print(f\"  - Classifier: {type(results['classifier']).__name__}\")\n",
    "    \n",
    "    # Show pipeline structure\n",
    "    pipeline = results['pipeline']\n",
    "    print(f\"\\n  🔧 Pipeline Structure:\")\n",
    "    for i, (name, step) in enumerate(pipeline.steps):\n",
    "        print(f\"    {i+1}. {name}: {type(step).__name__}\")\n",
    "    \n",
    "    print(f\"\\n  💡 Pipeline Benefits:\")\n",
    "    print(f\"    • Prevents data leakage (scaler fit only on training data)\")\n",
    "    print(f\"    • Ensures consistent preprocessing\")\n",
    "    print(f\"    • Simplifies prediction workflow\")\n",
    "    print(f\"    • Easy to serialize and deploy\")\n",
    "    \n",
    "    print(f\"\\n  🎯 Key Methods:\")\n",
    "    print(f\"    • fit(): Fits all steps on training data\")\n",
    "    print(f\"    • predict(): Transforms data through all steps, predicts with final\")\n",
    "    print(f\"    • named_steps: Access individual pipeline components\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 14.2: ColumnTransformer - Feature-Specific Preprocessing\n",
    "**Objective**: Use ColumnTransformer to apply different preprocessing to different columns  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Real datasets often have mixed data types requiring different preprocessing. ColumnTransformer lets you apply specific transformations to specific columns.\n",
    "\n",
    "**Key Concept**: ColumnTransformer applies different transformers to different subsets of columns, then combines results. Essential for mixed-type datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_transformer_pipeline():\n",
    "    \"\"\"\n",
    "    Create a pipeline using ColumnTransformer for mixed data types.\n",
    "    We'll create a synthetic dataset with numeric and categorical features.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Pipeline results with mixed data preprocessing\n",
    "    \"\"\"\n",
    "    # Create a mixed dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Numeric features (different scales)\n",
    "    numeric_data = np.random.randn(n_samples, 3)\n",
    "    numeric_data[:, 0] *= 100  # Different scale\n",
    "    numeric_data[:, 1] *= 10   # Different scale\n",
    "    \n",
    "    # Categorical features\n",
    "    categories = ['A', 'B', 'C']\n",
    "    cat_data = np.random.choice(categories, size=(n_samples, 2))\n",
    "    \n",
    "    # Combine into DataFrame\n",
    "    df = pd.DataFrame(numeric_data, columns=['num1', 'num2', 'num3'])\n",
    "    df['cat1'] = cat_data[:, 0] \n",
    "    df['cat2'] = cat_data[:, 1]\n",
    "    \n",
    "    # Create target (classification)\n",
    "    y = (df['num1'] + df['num2'] > 0).astype(int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Define column names for different transformations\n",
    "    numeric_columns = None  # ['num1', 'num2', 'num3']\n",
    "    categorical_columns = None  # ['cat1', 'cat2']\n",
    "    \n",
    "    # TODO: Create ColumnTransformer\n",
    "    # Use StandardScaler for numeric columns, OneHotEncoder for categorical\n",
    "    preprocessor = None\n",
    "    # ColumnTransformer([\n",
    "    #     ('num', StandardScaler(), numeric_columns),\n",
    "    #     ('cat', OneHotEncoder(drop='first'), categorical_columns)\n",
    "    # ])\n",
    "    \n",
    "    # TODO: Create pipeline with preprocessor and classifier\n",
    "    pipeline = None\n",
    "    # Pipeline([\n",
    "    #     ('preprocessor', preprocessor),\n",
    "    #     ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    # ])\n",
    "    \n",
    "    # TODO: Fit pipeline and calculate accuracy\n",
    "    # pipeline.fit(X_train, y_train)\n",
    "    # accuracy = pipeline.score(X_test, y_test)\n",
    "    accuracy = None\n",
    "    \n",
    "    return {\n",
    "        'pipeline': pipeline,\n",
    "        'preprocessor': preprocessor,\n",
    "        'accuracy': accuracy,\n",
    "        'numeric_columns': numeric_columns,\n",
    "        'categorical_columns': categorical_columns,\n",
    "        'n_numeric': len(numeric_columns) if numeric_columns else 0,\n",
    "        'n_categorical': len(categorical_columns) if categorical_columns else 0\n",
    "    }\n",
    "\n",
    "@validator.koan(2, \"ColumnTransformer - Feature-Specific Preprocessing\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = create_column_transformer_pipeline()\n",
    "    \n",
    "    assert results['pipeline'] is not None, \"Pipeline not created\"\n",
    "    assert results['preprocessor'] is not None, \"Preprocessor not created\" \n",
    "    assert results['accuracy'] is not None, \"Accuracy not calculated\"\n",
    "    assert results['n_numeric'] == 3, f\"Should have 3 numeric columns, got {results['n_numeric']}\"\n",
    "    assert results['n_categorical'] == 2, f\"Should have 2 categorical columns, got {results['n_categorical']}\"\n",
    "    assert 0.7 <= results['accuracy'] <= 1.0, f\"Accuracy should be reasonable, got {results['accuracy']:.3f}\"\n",
    "    \n",
    "    print(\"✓ ColumnTransformer pipeline created successfully!\")\n",
    "    print(f\"  - Test accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"  - Numeric columns: {results['n_numeric']} (scaled)\")\n",
    "    print(f\"  - Categorical columns: {results['n_categorical']} (one-hot encoded)\")\n",
    "    \n",
    "    # Show preprocessor structure\n",
    "    preprocessor = results['preprocessor']\n",
    "    print(f\"\\n  🔄 Preprocessing Steps:\")\n",
    "    for name, transformer, columns in preprocessor.transformers:\n",
    "        print(f\"    {name}: {type(transformer).__name__} → {columns}\")\n",
    "    \n",
    "    # Test the preprocessing\n",
    "    pipeline = results['pipeline']\n",
    "    if pipeline:\n",
    "        # Get transformed feature names (if available)\n",
    "        try:\n",
    "            feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "            print(f\"\\n  📊 Transformed Features: {len(feature_names)} total\")\n",
    "            print(f\"    Sample names: {feature_names[:5]}...\")\n",
    "        except:\n",
    "            print(f\"\\n  📊 Feature transformation completed\")\n",
    "    \n",
    "    print(f\"\\n  💡 ColumnTransformer Benefits:\")\n",
    "    print(f\"    • Different preprocessing for different data types\")\n",
    "    print(f\"    • Automatic feature concatenation\")\n",
    "    print(f\"    • Maintains column-specific transformations\")\n",
    "    print(f\"    • Handles mixed datasets elegantly\")\n",
    "    \n",
    "    print(f\"\\n  🎯 Common Use Cases:\")\n",
    "    print(f\"    • Scale numeric, encode categorical\")\n",
    "    print(f\"    • Different imputation strategies per column type\")\n",
    "    print(f\"    • Feature selection on subsets\")\n",
    "    print(f\"    • Custom transformations per feature group\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 14.3: Custom Transformers - Extending scikit-learn\n",
    "**Objective**: Create custom transformer classes for domain-specific preprocessing  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Sometimes you need preprocessing logic that isn't available in scikit-learn. Custom transformers let you integrate domain-specific transformations into pipelines.\n",
    "\n",
    "**Key Concept**: Custom transformers inherit from BaseEstimator and TransformerMixin, implementing fit() and transform() methods to work seamlessly in pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineeringTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer that creates new features from existing ones.\n",
    "    \n",
    "    This example creates polynomial features and interaction terms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, create_interactions=True, create_polynomials=True):\n",
    "        self.create_interactions = create_interactions\n",
    "        self.create_polynomials = create_polynomials\n",
    "        self.feature_names_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformer. For this transformer, we just store feature names.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y: Target (unused)\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # TODO: Store original feature names for later use\n",
    "        if hasattr(X, 'columns'):\n",
    "            self.feature_names_ = list(X.columns)\n",
    "        else:\n",
    "            self.feature_names_ = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input by creating new engineered features.\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "        Returns:\n",
    "            X_transformed: Original features plus engineered features\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame if needed\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=self.feature_names_)\n",
    "        \n",
    "        X_new = X.copy()\n",
    "        \n",
    "        # TODO: Create polynomial features (squares) if enabled\n",
    "        if self.create_polynomials:\n",
    "            for col in self.feature_names_:\n",
    "                if pd.api.types.is_numeric_dtype(X_new[col]):\n",
    "                    # Create squared feature\n",
    "                    pass  # X_new[f'{col}_squared'] = X_new[col] ** 2\n",
    "        \n",
    "        # TODO: Create interaction features if enabled  \n",
    "        if self.create_interactions:\n",
    "            numeric_cols = [col for col in self.feature_names_ \n",
    "                          if pd.api.types.is_numeric_dtype(X_new[col])]\n",
    "            \n",
    "            # Create interactions between first few numeric columns (to avoid explosion)\n",
    "            for i, col1 in enumerate(numeric_cols[:3]):\n",
    "                for col2 in numeric_cols[i+1:4]:  # Limit interactions\n",
    "                    # Create interaction feature\n",
    "                    pass  # X_new[f'{col1}_x_{col2}'] = X_new[col1] * X_new[col2]\n",
    "        \n",
    "        return X_new.values if not isinstance(X, pd.DataFrame) else X_new\n",
    "\n",
    "def test_custom_transformer():\n",
    "    \"\"\"\n",
    "    Test the custom transformer in a complete pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results from pipeline with custom transformer\n",
    "    \"\"\"\n",
    "    # Load wine dataset\n",
    "    wine = load_wine()\n",
    "    X, y = wine.data, wine.target\n",
    "    \n",
    "    # Convert to DataFrame for easier feature engineering\n",
    "    X_df = pd.DataFrame(X, columns=wine.feature_names)\n",
    "    \n",
    "    # Use only first 5 features to keep it manageable\n",
    "    X_subset = X_df.iloc[:, :5]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_subset, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Create pipeline with custom transformer\n",
    "    pipeline = None\n",
    "    # Pipeline([\n",
    "    #     ('feature_eng', FeatureEngineeringTransformer(create_interactions=True, create_polynomials=True)),\n",
    "    #     ('scaler', StandardScaler()),\n",
    "    #     ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    # ])\n",
    "    \n",
    "    # TODO: Fit pipeline and calculate accuracy\n",
    "    # pipeline.fit(X_train, y_train)\n",
    "    # accuracy = pipeline.score(X_test, y_test)\n",
    "    accuracy = None\n",
    "    \n",
    "    # Compare with baseline (no feature engineering)\n",
    "    baseline_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ])\n",
    "    baseline_pipeline.fit(X_train, y_train)\n",
    "    baseline_accuracy = baseline_pipeline.score(X_test, y_test)\n",
    "    \n",
    "    # Get feature count after engineering\n",
    "    if pipeline:\n",
    "        feature_eng = pipeline.named_steps['feature_eng']\n",
    "        X_transformed = feature_eng.transform(X_train)\n",
    "        n_engineered_features = X_transformed.shape[1]\n",
    "    else:\n",
    "        n_engineered_features = 0\n",
    "    \n",
    "    return {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': accuracy,\n",
    "        'baseline_accuracy': baseline_accuracy,\n",
    "        'improvement': (accuracy - baseline_accuracy) if accuracy else 0,\n",
    "        'original_features': X_subset.shape[1],\n",
    "        'engineered_features': n_engineered_features\n",
    "    }\n",
    "\n",
    "@validator.koan(3, \"Custom Transformers - Extending scikit-learn\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = test_custom_transformer()\n",
    "    \n",
    "    assert results['pipeline'] is not None, \"Pipeline with custom transformer not created\"\n",
    "    assert results['accuracy'] is not None, \"Accuracy not calculated\"\n",
    "    assert results['engineered_features'] > results['original_features'], \"Should create additional features\"\n",
    "    assert 0.7 <= results['accuracy'] <= 1.0, f\"Accuracy should be reasonable, got {results['accuracy']:.3f}\"\n",
    "    assert 0.7 <= results['baseline_accuracy'] <= 1.0, f\"Baseline should be reasonable, got {results['baseline_accuracy']:.3f}\"\n",
    "    \n",
    "    print(\"✓ Custom transformer pipeline created successfully!\")\n",
    "    print(f\"  - Original features: {results['original_features']}\")\n",
    "    print(f\"  - Engineered features: {results['engineered_features']}\")\n",
    "    print(f\"  - Feature expansion: +{results['engineered_features'] - results['original_features']} features\")\n",
    "    \n",
    "    print(f\"\\n  📊 Performance Comparison:\")\n",
    "    print(f\"    Baseline (no engineering): {results['baseline_accuracy']:.4f}\")\n",
    "    print(f\"    With feature engineering: {results['accuracy']:.4f}\")\n",
    "    print(f\"    Improvement: {results['improvement']:+.4f}\")\n",
    "    \n",
    "    if results['improvement'] > 0.01:\n",
    "        print(f\"    🎉 Significant improvement from feature engineering!\")\n",
    "    elif results['improvement'] > -0.01:\n",
    "        print(f\"    ✓ Feature engineering didn't hurt performance\")\n",
    "    else:\n",
    "        print(f\"    ⚠️ Feature engineering may have added noise\")\n",
    "    \n",
    "    print(f\"\\n  🛠️ Custom Transformer Requirements:\")\n",
    "    print(f\"    • Inherit from BaseEstimator, TransformerMixin\")\n",
    "    print(f\"    • Implement fit(X, y=None) method\")\n",
    "    print(f\"    • Implement transform(X) method\")\n",
    "    print(f\"    • Return self from fit() for method chaining\")\n",
    "    \n",
    "    print(f\"\\n  💡 Custom Transformer Use Cases:\")\n",
    "    print(f\"    • Domain-specific feature engineering\")\n",
    "    print(f\"    • Business rule transformations\")\n",
    "    print(f\"    • Complex data cleaning logic\")\n",
    "    print(f\"    • Integration with external APIs\")\n",
    "    print(f\"    • Time series feature extraction\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 14.4: Pipeline with GridSearch - Integrated Hyperparameter Tuning\n",
    "**Objective**: Combine pipelines with GridSearchCV to tune both preprocessing and model parameters  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "One of the most powerful features of pipelines is the ability to tune preprocessing parameters alongside model parameters, preventing data leakage and finding optimal combinations.\n",
    "\n",
    "**Key Concept**: Use double underscores (__) to specify parameters for specific pipeline steps in GridSearchCV parameter grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_with_gridsearch():\n",
    "    \"\"\"\n",
    "    Create a pipeline and use GridSearchCV to tune both preprocessing and model parameters.\n",
    "    \n",
    "    Returns:\n",
    "        dict: GridSearch results with best pipeline configuration\n",
    "    \"\"\"\n",
    "    # Load breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Create pipeline with multiple preprocessing options\n",
    "    pipeline = None\n",
    "    # Pipeline([\n",
    "    #     ('scaler', StandardScaler()),  # This will be tuned\n",
    "    #     ('classifier', SVC(random_state=42))  # This will be tuned\n",
    "    # ])\n",
    "    \n",
    "    # TODO: Define parameter grid for pipeline tuning\n",
    "    # Use step_name__parameter_name format\n",
    "    param_grid = [\n",
    "        {\n",
    "            'scaler': None,  # [StandardScaler(), MinMaxScaler()]\n",
    "            'classifier__C': None,  # [0.1, 1, 10]\n",
    "            'classifier__kernel': None,  # ['linear', 'rbf']\n",
    "            'classifier__gamma': None   # ['scale', 'auto']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # TODO: Create GridSearchCV for the pipeline\n",
    "    grid_search = None\n",
    "    # GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    # TODO: Fit the grid search\n",
    "    # grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate best pipeline\n",
    "    best_score = grid_search.best_score_ if grid_search else 0\n",
    "    test_score = grid_search.score(X_test, y_test) if grid_search else 0\n",
    "    best_params = grid_search.best_params_ if grid_search else {}\n",
    "    \n",
    "    return {\n",
    "        'grid_search': grid_search,\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'test_score': test_score,\n",
    "        'best_pipeline': grid_search.best_estimator_ if grid_search else None,\n",
    "        'n_combinations': len(grid_search.cv_results_['params']) if grid_search else 0\n",
    "    }\n",
    "\n",
    "@validator.koan(4, \"Pipeline with GridSearch - Integrated Hyperparameter Tuning\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = pipeline_with_gridsearch()\n",
    "    \n",
    "    assert results['grid_search'] is not None, \"GridSearch not created\"\n",
    "    assert results['best_params'] is not None, \"Best parameters not found\"\n",
    "    assert results['best_cv_score'] > 0, \"Best CV score not calculated\"\n",
    "    assert results['test_score'] > 0, \"Test score not calculated\"\n",
    "    assert results['n_combinations'] > 0, \"No parameter combinations tested\"\n",
    "    assert 0.85 <= results['best_cv_score'] <= 1.0, f\"CV score should be good, got {results['best_cv_score']:.3f}\"\n",
    "    \n",
    "    print(\"✓ Pipeline GridSearch optimization complete!\")\n",
    "    print(f\"  - Parameter combinations tested: {results['n_combinations']}\")\n",
    "    print(f\"  - Best CV score: {results['best_cv_score']:.4f}\")\n",
    "    print(f\"  - Test score: {results['test_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  🏆 Best Configuration:\")\n",
    "    for param, value in results['best_params'].items():\n",
    "        step, param_name = param.split('__') if '__' in param else ('pipeline', param)\n",
    "        print(f\"    {step} {param_name}: {type(value).__name__ if hasattr(value, '__name__') else value}\")\n",
    "    \n",
    "    # Check generalization\n",
    "    cv_test_diff = abs(results['best_cv_score'] - results['test_score'])\n",
    "    if cv_test_diff < 0.02:\n",
    "        print(f\"\\n  ✓ Excellent generalization (CV-Test diff: {cv_test_diff:.3f})\")\n",
    "    elif cv_test_diff < 0.05:\n",
    "        print(f\"\\n  ✓ Good generalization (CV-Test diff: {cv_test_diff:.3f})\")\n",
    "    else:\n",
    "        print(f\"\\n  ⚠️ Possible overfitting (CV-Test diff: {cv_test_diff:.3f})\")\n",
    "    \n",
    "    print(f\"\\n  🔧 Pipeline Parameter Tuning:\")\n",
    "    print(f\"    • Use step_name__parameter_name syntax\")\n",
    "    print(f\"    • Can tune preprocessing AND model parameters\")\n",
    "    print(f\"    • Prevents data leakage automatically\")\n",
    "    print(f\"    • Finds optimal preprocessing-model combinations\")\n",
    "    \n",
    "    print(f\"\\n  💡 Advanced Tuning Tips:\")\n",
    "    print(f\"    • Try different scalers (Standard, MinMax, Robust)\")\n",
    "    print(f\"    • Tune feature selection parameters\")\n",
    "    print(f\"    • Include different algorithms in same grid\")\n",
    "    print(f\"    • Use nested CV for unbiased evaluation\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 14.5: Production Pipeline - Deployment-Ready Workflow\n",
    "**Objective**: Create a complete, production-ready ML pipeline with serialization  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Production pipelines must handle missing data, unknown categories, feature validation, and be serializable for deployment. This final koan brings together all pipeline concepts.\n",
    "\n",
    "**Key Concept**: Production pipelines need robust error handling, data validation, versioning, and the ability to be saved/loaded for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_production_pipeline():\n",
    "    \"\"\"\n",
    "    Create a comprehensive, production-ready ML pipeline.\n",
    "    \n",
    "    Handles: missing values, mixed data types, feature engineering, \n",
    "    model training, serialization, and prediction.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Complete production pipeline results\n",
    "    \"\"\"\n",
    "    # Create realistic dataset with missing values and mixed types\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    # Create mixed dataset with missing values\n",
    "    data = {\n",
    "        'numeric_1': np.random.randn(n_samples),\n",
    "        'numeric_2': np.random.randn(n_samples) * 10,\n",
    "        'numeric_3': np.random.randn(n_samples) * 100,\n",
    "        'category_1': np.random.choice(['A', 'B', 'C', None], n_samples, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "        'category_2': np.random.choice(['X', 'Y', 'Z'], n_samples),\n",
    "        'binary_feature': np.random.choice([0, 1], n_samples)\n",
    "    }\n",
    "    \n",
    "    # Add some missing values to numeric features\n",
    "    missing_mask = np.random.random(n_samples) < 0.05\n",
    "    data['numeric_1'][missing_mask] = np.nan\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    y = (df['numeric_1'].fillna(0) + df['numeric_2'] > 0).astype(int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Define column groups\n",
    "    numeric_columns = None  # ['numeric_1', 'numeric_2', 'numeric_3']\n",
    "    categorical_columns = None  # ['category_1', 'category_2'] \n",
    "    binary_columns = None  # ['binary_feature']\n",
    "    \n",
    "    # TODO: Create comprehensive preprocessor\n",
    "    preprocessor = None\n",
    "    # ColumnTransformer([\n",
    "    #     ('num', Pipeline([\n",
    "    #         ('imputer', SimpleImputer(strategy='median')),\n",
    "    #         ('scaler', StandardScaler())\n",
    "    #     ]), numeric_columns),\n",
    "    #     ('cat', Pipeline([\n",
    "    #         ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    #         ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "    #     ]), categorical_columns),\n",
    "    #     ('bin', 'passthrough', binary_columns)  # Keep binary as-is\n",
    "    # ])\n",
    "    \n",
    "    # TODO: Create full production pipeline\n",
    "    production_pipeline = None\n",
    "    # Pipeline([\n",
    "    #     ('preprocessor', preprocessor),\n",
    "    #     ('feature_selection', 'passthrough'),  # Could add feature selection\n",
    "    #     ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "    # ])\n",
    "    \n",
    "    # TODO: Fit the pipeline\n",
    "    # production_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    train_score = production_pipeline.score(X_train, y_train) if production_pipeline else 0\n",
    "    test_score = production_pipeline.score(X_test, y_test) if production_pipeline else 0\n",
    "    \n",
    "    # TODO: Save pipeline for deployment\n",
    "    pipeline_filename = 'production_model.joblib'\n",
    "    # joblib.dump(production_pipeline, pipeline_filename)\n",
    "    \n",
    "    # TODO: Load pipeline (simulate deployment)\n",
    "    # loaded_pipeline = joblib.load(pipeline_filename)\n",
    "    loaded_pipeline = production_pipeline  # For validation purposes\n",
    "    \n",
    "    # Test loaded pipeline\n",
    "    loaded_test_score = loaded_pipeline.score(X_test, y_test) if loaded_pipeline else 0\n",
    "    \n",
    "    return {\n",
    "        'pipeline': production_pipeline,\n",
    "        'loaded_pipeline': loaded_pipeline,\n",
    "        'train_score': train_score,\n",
    "        'test_score': test_score, \n",
    "        'loaded_test_score': loaded_test_score,\n",
    "        'numeric_cols': len(numeric_columns) if numeric_columns else 0,\n",
    "        'categorical_cols': len(categorical_columns) if categorical_columns else 0,\n",
    "        'binary_cols': len(binary_columns) if binary_columns else 0,\n",
    "        'serialization_works': abs(test_score - loaded_test_score) < 0.001 if test_score and loaded_test_score else False\n",
    "    }\n",
    "\n",
    "@validator.koan(5, \"Production Pipeline - Deployment-Ready Workflow\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = create_production_pipeline()\n",
    "    \n",
    "    assert results['pipeline'] is not None, \"Production pipeline not created\"\n",
    "    assert results['loaded_pipeline'] is not None, \"Pipeline serialization/loading failed\"\n",
    "    assert results['train_score'] > 0, \"Training score not calculated\"\n",
    "    assert results['test_score'] > 0, \"Test score not calculated\"\n",
    "    assert results['numeric_cols'] == 3, f\"Should handle 3 numeric columns, got {results['numeric_cols']}\"\n",
    "    assert results['categorical_cols'] == 2, f\"Should handle 2 categorical columns, got {results['categorical_cols']}\"\n",
    "    assert results['binary_cols'] == 1, f\"Should handle 1 binary column, got {results['binary_cols']}\"\n",
    "    assert 0.7 <= results['test_score'] <= 1.0, f\"Test score should be reasonable, got {results['test_score']:.3f}\"\n",
    "    \n",
    "    print(\"✓ Production pipeline created successfully!\")\n",
    "    print(f\"  - Training accuracy: {results['train_score']:.4f}\")\n",
    "    print(f\"  - Test accuracy: {results['test_score']:.4f}\")\n",
    "    print(f\"  - Loaded model accuracy: {results['loaded_test_score']:.4f}\")\n",
    "    \n",
    "    if results['serialization_works']:\n",
    "        print(f\"  ✓ Serialization working correctly\")\n",
    "    else:\n",
    "        print(f\"  ⚠️ Serialization may have issues\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    train_test_diff = results['train_score'] - results['test_score']\n",
    "    if train_test_diff < 0.05:\n",
    "        print(f\"  ✓ No overfitting detected (diff: {train_test_diff:.3f})\")\n",
    "    elif train_test_diff < 0.1:\n",
    "        print(f\"  ⚠️ Mild overfitting (diff: {train_test_diff:.3f})\")\n",
    "    else:\n",
    "        print(f\"  🚨 Significant overfitting (diff: {train_test_diff:.3f})\")\n",
    "    \n",
    "    print(f\"\\n  🏗️ Pipeline Architecture:\")\n",
    "    print(f\"    Numeric columns: {results['numeric_cols']} (impute → scale)\")\n",
    "    print(f\"    Categorical columns: {results['categorical_cols']} (impute → encode)\")\n",
    "    print(f\"    Binary columns: {results['binary_cols']} (passthrough)\")\n",
    "    \n",
    "    print(f\"\\n  🚀 Production Readiness Checklist:\")\n",
    "    print(f\"    ✅ Handles missing values gracefully\")\n",
    "    print(f\"    ✅ Handles unknown categories\")\n",
    "    print(f\"    ✅ Mixed data type support\")\n",
    "    print(f\"    ✅ Serialization for deployment\")\n",
    "    print(f\"    ✅ Consistent preprocessing pipeline\")\n",
    "    print(f\"    ✅ No data leakage\")\n",
    "    \n",
    "    print(f\"\\n  📦 Deployment Considerations:\")\n",
    "    print(f\"    • Version your pipelines (model versioning)\")\n",
    "    print(f\"    • Monitor feature distributions in production\")\n",
    "    print(f\"    • Handle feature drift and concept drift\")  \n",
    "    print(f\"    • Implement A/B testing for model updates\")\n",
    "    print(f\"    • Add input validation and error handling\")\n",
    "    print(f\"    • Consider batch vs. real-time prediction needs\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "You have mastered ML Pipelines - the foundation of production machine learning!\n",
    "\n",
    "### What You've Built\n",
    "- ✅ **Basic Pipelines**: Automated preprocessing + modeling workflows\n",
    "- ✅ **ColumnTransformer**: Mixed data type preprocessing strategies\n",
    "- ✅ **Custom Transformers**: Domain-specific transformation logic\n",
    "- ✅ **Integrated Tuning**: Combined preprocessing and model optimization  \n",
    "- ✅ **Production Pipelines**: Deployment-ready ML workflows\n",
    "\n",
    "### Critical Skills Gained\n",
    "1. **Data Leakage Prevention**: Proper train/test separation in preprocessing\n",
    "2. **Workflow Automation**: End-to-end ML pipelines  \n",
    "3. **Robust Preprocessing**: Handling missing values, mixed types, unknowns\n",
    "4. **Hyperparameter Integration**: Tuning preprocessing + model together\n",
    "5. **Deployment Readiness**: Serializable, versioned, production workflows\n",
    "\n",
    "### Real-World Impact\n",
    "- 🛡️ **Eliminates 90%** of data leakage bugs in ML projects\n",
    "- ⚡ **10x faster** development with reusable pipeline components\n",
    "- 🎯 **Better models** through integrated hyperparameter tuning\n",
    "- 🚀 **Seamless deployment** with serialized pipeline objects\n",
    "- 📈 **Maintainable ML** with standardized workflows\n",
    "\n",
    "### Next Steps\n",
    "- **Notebook 15**: Ethics and Bias (responsible ML practices!)\n",
    "- **Advanced**: MLOps, model monitoring, automated retraining\n",
    "- **Practice**: Build pipelines for your own ML projects\n",
    "\n",
    "### Production Pipeline Checklist\n",
    "- ✅ Input validation and error handling\n",
    "- ✅ Missing value imputation strategies  \n",
    "- ✅ Unknown category handling\n",
    "- ✅ Feature scaling and encoding\n",
    "- ✅ Model serialization/versioning\n",
    "- ✅ Consistent preprocessing logic\n",
    "- ✅ Performance monitoring hooks\n",
    "- ✅ A/B testing capabilities\n",
    "\n",
    "You're now ready to build production-grade ML systems! 🏭✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Progress Check\n",
    "progress = tracker.get_notebook_progress('14_pipelines')\n",
    "print(f\"\\n📊 Your Progress: {progress}% complete!\")\n",
    "\n",
    "if progress == 100:\n",
    "    print(\"🎉 Phenomenal! You've mastered all pipeline techniques!\")\n",
    "    print(\"🎯 Ready for Notebook 15: Ethics and Bias (the final frontier!)\")\n",
    "elif progress >= 75:\n",
    "    print(\"🌟 Excellent progress! Pipeline mastery is within reach.\")\n",
    "elif progress >= 50:\n",
    "    print(\"💪 Great work! You're building production-ready skills.\")\n",
    "else:\n",
    "    print(\"🚀 Keep going! Each pipeline concept builds toward deployment readiness.\")\n",
    "\n",
    "print(f\"\\n📈 Overall course progress:\")\n",
    "total_notebooks = 15\n",
    "completed_notebooks = len([nb for nb in range(1, 15) if tracker.get_notebook_progress(f'{nb:02d}_*') == 100])\n",
    "print(f\"   Completed notebooks: {completed_notebooks}/{total_notebooks}\")\n",
    "print(f\"   Course progress: {(completed_notebooks/total_notebooks)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n🏭 Production ML Pipeline Mastery Achieved!\")\n",
    "print(f\"   You can now build enterprise-grade ML systems! 🎖️\")\n",
    "\n",
    "if completed_notebooks >= 14:\n",
    "    print(f\"\\n🎊 Almost finished! One more notebook to complete your\")  \n",
    "    print(f\"   Data Science Koans journey: Ethics and Bias! 🧭\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
