{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods - Data Science Koans\n",
    "\n",
    "Welcome to Notebook 12: Ensemble Methods!\n",
    "\n",
    "## What You Will Learn\n",
    "- Random Forest fundamentals and implementation\n",
    "- Feature importance analysis and interpretation  \n",
    "- Gradient Boosting and XGBoost techniques\n",
    "- Voting classifiers for model combination\n",
    "- Stacking ensembles for meta-learning\n",
    "- Comparing and evaluating ensemble methods\n",
    "\n",
    "## Why This Matters\n",
    "Ensemble methods are among the most powerful techniques in machine learning because they:\n",
    "- **Reduce Overfitting**: Multiple models average out individual errors\n",
    "- **Improve Accuracy**: Combine strengths of different algorithms\n",
    "- **Increase Robustness**: Less sensitive to outliers and noise\n",
    "- **Handle Complexity**: Capture non-linear patterns effectively\n",
    "- **Win Competitions**: Dominate Kaggle and real-world challenges\n",
    "\n",
    "## Key Concepts\n",
    "- **Bagging**: Train multiple models on different data subsets (Random Forest)\n",
    "- **Boosting**: Train models sequentially, focusing on previous errors (XGBoost)  \n",
    "- **Voting**: Combine predictions through majority vote or averaging\n",
    "- **Stacking**: Use a meta-model to learn optimal combination weights\n",
    "\n",
    "## Prerequisites\n",
    "- Dimensionality Reduction (Notebook 11)\n",
    "- Understanding of classification and regression\n",
    "- Experience with scikit-learn\n",
    "\n",
    "## How to Use\n",
    "1. Read each koan's objective and theory explanation\n",
    "2. Implement the TODO sections step by step\n",
    "3. Run validation to verify correctness\n",
    "4. Study the feedback and insights provided\n",
    "5. Progress through increasingly advanced ensemble techniques\n",
    "\n",
    "Ready to build some powerful ensemble models? Let's go! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Run first!\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer, load_wine, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor, \n",
    "                              GradientBoostingClassifier, VotingClassifier, \n",
    "                              StackingClassifier)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# XGBoost (may need installation: pip install xgboost)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print(\"‚úì XGBoost available\")\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è XGBoost not available - will use GradientBoostingClassifier instead\")\n",
    "\n",
    "from koans.core.validator import KoanValidator\n",
    "from koans.core.progress import ProgressTracker\n",
    "\n",
    "validator = KoanValidator(\"12_ensemble_methods\")\n",
    "tracker = ProgressTracker()\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Current progress: {tracker.get_notebook_progress('12_ensemble_methods')}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 12.1: Random Forest Classifier\n",
    "**Objective**: Build and train a Random Forest for classification  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Random Forest combines many decision trees using bagging (bootstrap aggregating). Each tree trains on a random subset of data and features, then predictions are averaged (regression) or voted (classification).\n",
    "\n",
    "**Key Concepts**: \n",
    "- **Bootstrap Sampling**: Each tree sees different training data\n",
    "- **Feature Randomness**: Each split considers random subset of features  \n",
    "- **Averaging**: Multiple weak learners create a strong ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_random_forest_classifier():\n",
    "    \"\"\"\n",
    "    Build and train a Random Forest classifier on the breast cancer dataset.\n",
    "    \n",
    "    Steps:\n",
    "    1. Load the breast cancer dataset\n",
    "    2. Split into training and test sets\n",
    "    3. Create RandomForestClassifier with 100 trees\n",
    "    4. Train the model\n",
    "    5. Return the trained model and test accuracy\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trained_model, test_accuracy)\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Create RandomForestClassifier with n_estimators=100, random_state=42\n",
    "    rf_model = None\n",
    "    \n",
    "    # TODO: Fit the model on training data\n",
    "    # rf_model.fit(...)\n",
    "    \n",
    "    # TODO: Make predictions and calculate accuracy\n",
    "    # y_pred = rf_model.predict(...)\n",
    "    # accuracy = accuracy_score(...)\n",
    "    accuracy = None\n",
    "    \n",
    "    return rf_model, accuracy\n",
    "\n",
    "@validator.koan(1, \"Random Forest Classifier\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    model, accuracy = build_random_forest_classifier()\n",
    "    \n",
    "    assert model is not None, \"Model is None - did you create the RandomForestClassifier?\"\n",
    "    assert isinstance(model, RandomForestClassifier), \"Model should be RandomForestClassifier\"\n",
    "    assert hasattr(model, 'n_estimators'), \"Model should have n_estimators attribute\"\n",
    "    assert model.n_estimators == 100, f\"Expected 100 estimators, got {model.n_estimators}\"\n",
    "    \n",
    "    assert accuracy is not None, \"Accuracy is None - did you calculate it?\"\n",
    "    assert isinstance(accuracy, (float, np.floating)), \"Accuracy should be a float\"\n",
    "    assert 0.8 <= accuracy <= 1.0, f\"Accuracy should be reasonable (0.8-1.0), got {accuracy:.3f}\"\n",
    "    \n",
    "    print(f\"‚úì Random Forest trained successfully!\")\n",
    "    print(f\"  - Number of trees: {model.n_estimators}\")\n",
    "    print(f\"  - Test accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  - Features used: {model.n_features_in_}\")\n",
    "    \n",
    "    # Show feature importance preview\n",
    "    cancer = load_breast_cancer()\n",
    "    feature_names = cancer.feature_names\n",
    "    importances = model.feature_importances_\n",
    "    top_features = np.argsort(importances)[-3:][::-1]\n",
    "    \n",
    "    print(\"  - Top 3 important features:\")\n",
    "    for i, idx in enumerate(top_features):\n",
    "        print(f\"    {i+1}. {feature_names[idx]}: {importances[idx]:.3f}\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 12.2: Feature Importance Analysis\n",
    "**Objective**: Extract and analyze feature importance from Random Forest  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Random Forest provides built-in feature importance scores based on how much each feature decreases impurity across all trees. This helps identify which variables are most predictive.\n",
    "\n",
    "**Key Concept**: `feature_importances_` gives a score for each feature. Higher scores mean more important features. Scores sum to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance():\n",
    "    \"\"\"\n",
    "    Train Random Forest on wine dataset and analyze feature importance.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (feature_names, importance_scores, top_5_indices)\n",
    "        where top_5_indices are indices of 5 most important features\n",
    "    \"\"\"\n",
    "    # Load wine dataset\n",
    "    wine = load_wine()\n",
    "    X, y = wine.data, wine.target\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # TODO: Create and train RandomForestClassifier with 200 trees\n",
    "    rf_model = None\n",
    "    \n",
    "    # TODO: Get feature names and importance scores\n",
    "    feature_names = None  # wine.feature_names\n",
    "    importance_scores = None  # rf_model.feature_importances_\n",
    "    \n",
    "    # TODO: Find indices of top 5 most important features\n",
    "    # Hint: Use np.argsort()[-5:][::-1] to get top 5 in descending order\n",
    "    top_5_indices = None\n",
    "    \n",
    "    return feature_names, importance_scores, top_5_indices\n",
    "\n",
    "@validator.koan(2, \"Feature Importance Analysis\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    feature_names, importance_scores, top_5_indices = analyze_feature_importance()\n",
    "    \n",
    "    assert feature_names is not None, \"Feature names is None\"\n",
    "    assert importance_scores is not None, \"Importance scores is None\" \n",
    "    assert top_5_indices is not None, \"Top 5 indices is None\"\n",
    "    \n",
    "    assert len(feature_names) == 13, f\"Wine dataset should have 13 features, got {len(feature_names)}\"\n",
    "    assert len(importance_scores) == 13, f\"Should have 13 importance scores, got {len(importance_scores)}\"\n",
    "    assert len(top_5_indices) == 5, f\"Should have 5 top indices, got {len(top_5_indices)}\"\n",
    "    \n",
    "    # Check that importance scores sum to approximately 1\n",
    "    total_importance = np.sum(importance_scores)\n",
    "    assert abs(total_importance - 1.0) < 0.001, f\"Importances should sum to 1.0, got {total_importance:.3f}\"\n",
    "    \n",
    "    # Check that top indices are in descending order of importance\n",
    "    for i in range(4):\n",
    "        curr_imp = importance_scores[top_5_indices[i]]\n",
    "        next_imp = importance_scores[top_5_indices[i+1]]\n",
    "        assert curr_imp >= next_imp, \"Top indices should be in descending order of importance\"\n",
    "    \n",
    "    print(\"‚úì Feature importance analysis complete!\")\n",
    "    print(f\"  - Total features: {len(feature_names)}\")\n",
    "    print(f\"  - Importance sum: {total_importance:.3f}\")\n",
    "    \n",
    "    print(\"\\n  üèÜ Top 5 Most Important Features:\")\n",
    "    for i, idx in enumerate(top_5_indices):\n",
    "        print(f\"    {i+1}. {feature_names[idx]}: {importance_scores[idx]:.3f}\")\n",
    "    \n",
    "    # Additional insight\n",
    "    most_important = top_5_indices[0]\n",
    "    print(f\"\\n  üí° Most important feature: '{feature_names[most_important]}'\")\n",
    "    print(f\"     Contributes {importance_scores[most_important]*100:.1f}% to predictions\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 12.3: Gradient Boosting Classifier  \n",
    "**Objective**: Implement gradient boosting for sequential learning  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Gradient Boosting builds models sequentially, where each new model focuses on correcting errors from previous models. This creates a strong learner from many weak learners.\n",
    "\n",
    "**Key Concept**: Unlike bagging (Random Forest), boosting trains models in sequence. Each model tries to reduce the residual errors of the ensemble built so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_random_forest_vs_gradient_boosting():\n",
    "    \"\"\"\n",
    "    Compare Random Forest and Gradient Boosting on the same dataset.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains both models and their cross-validation scores\n",
    "    \"\"\"\n",
    "    # Create a synthetic dataset for comparison\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=20, n_informative=10, \n",
    "        n_redundant=10, random_state=42\n",
    "    )\n",
    "    \n",
    "    # TODO: Create RandomForestClassifier with 100 estimators, random_state=42\n",
    "    rf_model = None\n",
    "    \n",
    "    # TODO: Create GradientBoostingClassifier with 100 estimators, random_state=42  \n",
    "    gb_model = None\n",
    "    \n",
    "    # TODO: Calculate 5-fold cross-validation scores for both models\n",
    "    # Hint: Use cross_val_score(model, X, y, cv=5)\n",
    "    rf_scores = None\n",
    "    gb_scores = None\n",
    "    \n",
    "    return {\n",
    "        'rf_model': rf_model,\n",
    "        'gb_model': gb_model, \n",
    "        'rf_scores': rf_scores,\n",
    "        'gb_scores': gb_scores,\n",
    "        'rf_mean': np.mean(rf_scores) if rf_scores is not None else None,\n",
    "        'gb_mean': np.mean(gb_scores) if gb_scores is not None else None\n",
    "    }\n",
    "\n",
    "@validator.koan(3, \"Gradient Boosting Classifier\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = compare_random_forest_vs_gradient_boosting()\n",
    "    \n",
    "    # Check that models were created\n",
    "    assert results['rf_model'] is not None, \"Random Forest model is None\"\n",
    "    assert results['gb_model'] is not None, \"Gradient Boosting model is None\"\n",
    "    \n",
    "    # Check model types\n",
    "    assert isinstance(results['rf_model'], RandomForestClassifier), \"rf_model should be RandomForestClassifier\"\n",
    "    assert isinstance(results['gb_model'], GradientBoostingClassifier), \"gb_model should be GradientBoostingClassifier\"\n",
    "    \n",
    "    # Check cross-validation scores\n",
    "    assert results['rf_scores'] is not None, \"Random Forest CV scores is None\"\n",
    "    assert results['gb_scores'] is not None, \"Gradient Boosting CV scores is None\"\n",
    "    assert len(results['rf_scores']) == 5, \"Should have 5 CV scores for Random Forest\"\n",
    "    assert len(results['gb_scores']) == 5, \"Should have 5 CV scores for Gradient Boosting\"\n",
    "    \n",
    "    # Check that scores are reasonable\n",
    "    rf_mean = results['rf_mean']\n",
    "    gb_mean = results['gb_mean']\n",
    "    assert 0.7 <= rf_mean <= 1.0, f\"RF accuracy should be reasonable, got {rf_mean:.3f}\"\n",
    "    assert 0.7 <= gb_mean <= 1.0, f\"GB accuracy should be reasonable, got {gb_mean:.3f}\"\n",
    "    \n",
    "    print(\"‚úì Successfully compared Random Forest vs Gradient Boosting!\")\n",
    "    print(f\"\\n  üå≤ Random Forest Results:\")\n",
    "    print(f\"     Mean CV Accuracy: {rf_mean:.3f} (¬±{np.std(results['rf_scores']):.3f})\")\n",
    "    print(f\"     Individual scores: {[f'{s:.3f}' for s in results['rf_scores']]}\")\n",
    "    \n",
    "    print(f\"\\n  üöÄ Gradient Boosting Results:\")  \n",
    "    print(f\"     Mean CV Accuracy: {gb_mean:.3f} (¬±{np.std(results['gb_scores']):.3f})\")\n",
    "    print(f\"     Individual scores: {[f'{s:.3f}' for s in results['gb_scores']]}\")\n",
    "    \n",
    "    # Compare performance\n",
    "    if rf_mean > gb_mean:\n",
    "        winner = \"Random Forest\"\n",
    "        diff = rf_mean - gb_mean\n",
    "    else:\n",
    "        winner = \"Gradient Boosting\" \n",
    "        diff = gb_mean - rf_mean\n",
    "        \n",
    "    print(f\"\\n  üèÜ Winner: {winner} (by {diff:.3f})\")\n",
    "    print(f\"\\n  üí° Key Differences:\")\n",
    "    print(f\"     ‚Ä¢ Random Forest: Parallel training, faster\")\n",
    "    print(f\"     ‚Ä¢ Gradient Boosting: Sequential training, can overfit\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 12.4: XGBoost - Advanced Gradient Boosting\n",
    "**Objective**: Use XGBoost for state-of-the-art boosting performance  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is an optimized gradient boosting framework that often wins machine learning competitions. It includes regularization, handles missing values, and is highly optimized.\n",
    "\n",
    "**Key Concepts**: XGBoost adds regularization terms, uses second-order derivatives, and implements advanced techniques like tree pruning and parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_classifier():\n",
    "    \"\"\"\n",
    "    Train XGBoost classifier and compare with regular Gradient Boosting.\n",
    "    Falls back to GradientBoostingClassifier if XGBoost not available.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains model, accuracy, and model type info\n",
    "    \"\"\"\n",
    "    # Load breast cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    if XGBOOST_AVAILABLE:\n",
    "        # TODO: Create XGBClassifier with n_estimators=100, random_state=42\n",
    "        # Hint: Use xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "        model = None\n",
    "        model_type = \"XGBoost\"\n",
    "    else:\n",
    "        # Fallback to Gradient Boosting\n",
    "        # TODO: Create GradientBoostingClassifier with n_estimators=100, random_state=42\n",
    "        model = None\n",
    "        model_type = \"GradientBoosting\"\n",
    "    \n",
    "    # TODO: Train the model\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    # TODO: Calculate test accuracy\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy = None\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy, \n",
    "        'model_type': model_type,\n",
    "        'n_estimators': model.n_estimators if model else None\n",
    "    }\n",
    "\n",
    "@validator.koan(4, \"XGBoost - Advanced Gradient Boosting\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = train_xgboost_classifier()\n",
    "    \n",
    "    assert results['model'] is not None, \"Model is None\"\n",
    "    assert results['accuracy'] is not None, \"Accuracy is None\"\n",
    "    assert results['model_type'] in ['XGBoost', 'GradientBoosting'], \"Invalid model type\"\n",
    "    \n",
    "    model = results['model']\n",
    "    accuracy = results['accuracy']\n",
    "    \n",
    "    # Check model properties\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        assert hasattr(model, 'n_estimators'), \"XGBoost model should have n_estimators\"\n",
    "    else:\n",
    "        assert isinstance(model, GradientBoostingClassifier), \"Should be GradientBoostingClassifier\"\n",
    "        \n",
    "    assert results['n_estimators'] == 100, f\"Should have 100 estimators, got {results['n_estimators']}\"\n",
    "    assert 0.8 <= accuracy <= 1.0, f\"Accuracy should be reasonable, got {accuracy:.3f}\"\n",
    "    \n",
    "    print(f\"‚úì Successfully trained {results['model_type']} classifier!\")\n",
    "    print(f\"  - Model type: {results['model_type']}\")\n",
    "    print(f\"  - Number of estimators: {results['n_estimators']}\")\n",
    "    print(f\"  - Test accuracy: {accuracy:.3f}\")\n",
    "    \n",
    "    if results['model_type'] == 'XGBoost':\n",
    "        print(f\"  - XGBoost advantages:\")\n",
    "        print(f\"    ‚Ä¢ Built-in regularization\")\n",
    "        print(f\"    ‚Ä¢ Handles missing values\")  \n",
    "        print(f\"    ‚Ä¢ Optimized performance\")\n",
    "        print(f\"    ‚Ä¢ Feature importance\")\n",
    "    else:\n",
    "        print(f\"  - Using GradientBoostingClassifier (XGBoost not available)\")\n",
    "        print(f\"  - Install XGBoost: pip install xgboost\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 12.5: Voting Classifier - Combining Multiple Models\n",
    "**Objective**: Use VotingClassifier to combine different algorithms  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Voting classifiers combine predictions from multiple different algorithms. Hard voting uses majority vote, while soft voting averages predicted probabilities for potentially better performance.\n",
    "\n",
    "**Key Concept**: Diversity in base models is crucial - combining similar models provides little benefit, but combining different algorithm types (tree-based, linear, etc.) can improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_voting_ensemble():\n",
    "    \"\"\"\n",
    "    Create a voting classifier that combines multiple different algorithms.\n",
    "    \n",
    "    We'll combine: Random Forest, Logistic Regression, and SVM\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (voting_classifier, individual_scores, ensemble_score)\n",
    "    \"\"\"\n",
    "    # Load and prepare data\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features for SVM and Logistic Regression\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # TODO: Create individual base models\n",
    "    rf_model = None   # RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    lr_model = None   # LogisticRegression(random_state=42, max_iter=1000)  \n",
    "    svm_model = None  # SVC(random_state=42, probability=True)  # probability=True for soft voting\n",
    "    \n",
    "    # TODO: Create VotingClassifier with soft voting\n",
    "    # Hint: VotingClassifier([('rf', rf_model), ('lr', lr_model), ('svm', svm_model)], voting='soft')\n",
    "    voting_clf = None\n",
    "    \n",
    "    # Train individual models on scaled data (for fair comparison)\n",
    "    rf_model.fit(X_train_scaled, y_train) \n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    svm_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # TODO: Train the voting classifier\n",
    "    # voting_clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Calculate individual scores\n",
    "    rf_score = accuracy_score(y_test, rf_model.predict(X_test_scaled))\n",
    "    lr_score = accuracy_score(y_test, lr_model.predict(X_test_scaled))\n",
    "    svm_score = accuracy_score(y_test, svm_model.predict(X_test_scaled))\n",
    "    \n",
    "    # TODO: Calculate ensemble score\n",
    "    # ensemble_score = accuracy_score(y_test, voting_clf.predict(X_test_scaled))\n",
    "    ensemble_score = None\n",
    "    \n",
    "    individual_scores = {\n",
    "        'Random Forest': rf_score,\n",
    "        'Logistic Regression': lr_score, \n",
    "        'SVM': svm_score\n",
    "    }\n",
    "    \n",
    "    return voting_clf, individual_scores, ensemble_score\n",
    "\n",
    "@validator.koan(5, \"Voting Classifier - Combining Multiple Models\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    voting_clf, individual_scores, ensemble_score = create_voting_ensemble()\n",
    "    \n",
    "    assert voting_clf is not None, \"Voting classifier is None\"\n",
    "    assert isinstance(voting_clf, VotingClassifier), \"Should be VotingClassifier\"\n",
    "    assert individual_scores is not None, \"Individual scores is None\"\n",
    "    assert ensemble_score is not None, \"Ensemble score is None\"\n",
    "    \n",
    "    # Check that we have 3 base estimators\n",
    "    assert len(voting_clf.estimators) == 3, f\"Should have 3 estimators, got {len(voting_clf.estimators)}\"\n",
    "    \n",
    "    # Check voting type\n",
    "    assert voting_clf.voting == 'soft', f\"Should use soft voting, got {voting_clf.voting}\"\n",
    "    \n",
    "    # Check individual scores are reasonable\n",
    "    for name, score in individual_scores.items():\n",
    "        assert 0.7 <= score <= 1.0, f\"{name} score should be reasonable, got {score:.3f}\"\n",
    "    \n",
    "    # Check ensemble score is reasonable  \n",
    "    assert 0.7 <= ensemble_score <= 1.0, f\"Ensemble score should be reasonable, got {ensemble_score:.3f}\"\n",
    "    \n",
    "    print(\"‚úì Successfully created voting ensemble!\")\n",
    "    print(f\"  - Number of base models: {len(voting_clf.estimators)}\")\n",
    "    print(f\"  - Voting type: {voting_clf.voting}\")\n",
    "    \n",
    "    print(f\"\\n  üìä Individual Model Performance:\")\n",
    "    for name, score in individual_scores.items():\n",
    "        print(f\"    {name}: {score:.3f}\")\n",
    "    \n",
    "    print(f\"\\n  üó≥Ô∏è  Ensemble Performance: {ensemble_score:.3f}\")\n",
    "    \n",
    "    # Check if ensemble improves over individual models\n",
    "    best_individual = max(individual_scores.values())\n",
    "    improvement = ensemble_score - best_individual\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(f\"  üéâ Ensemble improved by {improvement:.3f} over best individual!\")\n",
    "    elif improvement > -0.01:  # Small decrease is acceptable\n",
    "        print(f\"  ‚úì Ensemble performs similarly to best individual\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Ensemble underperformed (can happen with small datasets)\")\n",
    "    \n",
    "    print(f\"\\n  üí° Voting Ensemble Benefits:\")\n",
    "    print(f\"    ‚Ä¢ Reduces overfitting through diversity\")  \n",
    "    print(f\"    ‚Ä¢ More robust predictions\")\n",
    "    print(f\"    ‚Ä¢ Soft voting can capture prediction confidence\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 12.6: Stacking Ensemble - Meta-Learning  \n",
    "**Objective**: Implement stacking with a meta-learner  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Stacking uses a meta-model (blender) to learn how to best combine predictions from base models. Instead of simple voting, the meta-model learns optimal weights based on base model performance patterns.\n",
    "\n",
    "**Key Concept**: Base models make predictions, then a meta-model learns from those predictions to make the final decision. This can capture complex interaction patterns between base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacking_ensemble():\n",
    "    \"\"\"\n",
    "    Create a stacking ensemble with base models and a meta-learner.\n",
    "    \n",
    "    Base models: Random Forest, SVM, Decision Tree\n",
    "    Meta-learner: Logistic Regression\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (stacking_classifier, base_scores, stacking_score)\n",
    "    \"\"\"\n",
    "    # Load and prepare data\n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # TODO: Define base models (estimators for stacking)\n",
    "    base_models = [\n",
    "        ('rf', None),   # ('rf', RandomForestClassifier(n_estimators=50, random_state=42))\n",
    "        ('svm', None),  # ('svm', SVC(random_state=42, probability=True))\n",
    "        ('dt', None)    # ('dt', DecisionTreeClassifier(random_state=42))\n",
    "    ]\n",
    "    \n",
    "    # TODO: Create StackingClassifier with LogisticRegression as meta-learner\n",
    "    # Hint: StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(), cv=5)\n",
    "    stacking_clf = None\n",
    "    \n",
    "    # TODO: Fit the stacking classifier\n",
    "    # stacking_clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Train individual base models for comparison\n",
    "    base_scores = {}\n",
    "    for name, model in base_models:\n",
    "        if model is not None:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            score = accuracy_score(y_test, model.predict(X_test_scaled))\n",
    "            base_scores[name] = score\n",
    "    \n",
    "    # TODO: Calculate stacking ensemble score\n",
    "    # stacking_score = accuracy_score(y_test, stacking_clf.predict(X_test_scaled))\n",
    "    stacking_score = None\n",
    "    \n",
    "    return stacking_clf, base_scores, stacking_score\n",
    "\n",
    "@validator.koan(6, \"Stacking Ensemble - Meta-Learning\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    stacking_clf, base_scores, stacking_score = create_stacking_ensemble()\n",
    "    \n",
    "    assert stacking_clf is not None, \"Stacking classifier is None\"\n",
    "    assert isinstance(stacking_clf, StackingClassifier), \"Should be StackingClassifier\"\n",
    "    assert base_scores is not None, \"Base scores is None\"\n",
    "    assert stacking_score is not None, \"Stacking score is None\"\n",
    "    \n",
    "    # Check that we have base estimators\n",
    "    assert len(stacking_clf.estimators) == 3, f\"Should have 3 base estimators, got {len(stacking_clf.estimators)}\"\n",
    "    \n",
    "    # Check final estimator\n",
    "    assert isinstance(stacking_clf.final_estimator, LogisticRegression), \"Meta-learner should be LogisticRegression\"\n",
    "    \n",
    "    # Check scores are reasonable\n",
    "    for name, score in base_scores.items():\n",
    "        assert 0.7 <= score <= 1.0, f\"{name} score should be reasonable, got {score:.3f}\"\n",
    "    \n",
    "    assert 0.7 <= stacking_score <= 1.0, f\"Stacking score should be reasonable, got {stacking_score:.3f}\"\n",
    "    \n",
    "    print(\"‚úì Successfully created stacking ensemble!\")\n",
    "    print(f\"  - Number of base models: {len(stacking_clf.estimators)}\")\n",
    "    print(f\"  - Meta-learner: {type(stacking_clf.final_estimator).__name__}\")\n",
    "    print(f\"  - Cross-validation folds: {stacking_clf.cv}\")\n",
    "    \n",
    "    print(f\"\\n  üìä Base Model Performance:\")\n",
    "    for name, score in base_scores.items():\n",
    "        print(f\"    {name.upper()}: {score:.3f}\")\n",
    "    \n",
    "    print(f\"\\n  ü•û Stacking Performance: {stacking_score:.3f}\")\n",
    "    \n",
    "    # Compare with best base model\n",
    "    best_base_score = max(base_scores.values())\n",
    "    improvement = stacking_score - best_base_score\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(f\"  üéâ Stacking improved by {improvement:.3f} over best base model!\")\n",
    "    elif improvement > -0.01:\n",
    "        print(f\"  ‚úì Stacking performs similarly to best base model\")  \n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Stacking underperformed (meta-learner may need tuning)\")\n",
    "    \n",
    "    print(f\"\\n  üí° Stacking Advantages:\")\n",
    "    print(f\"    ‚Ä¢ Meta-learner adapts to base model strengths\")\n",
    "    print(f\"    ‚Ä¢ Can learn complex combination patterns\") \n",
    "    print(f\"    ‚Ä¢ Often outperforms simple voting\")\n",
    "    print(f\"    ‚Ä¢ Cross-validation prevents overfitting\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KOAN 12.7: Ensemble Method Comparison\n",
    "**Objective**: Compare all ensemble methods on the same dataset  \n",
    "**Difficulty**: Advanced\n",
    "\n",
    "Now let's bring it all together! We'll compare Random Forest, Gradient Boosting, Voting, and Stacking ensembles to see their relative strengths and when each performs best.\n",
    "\n",
    "**Key Concept**: Different ensemble methods excel in different scenarios. Understanding their trade-offs helps choose the right approach for your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_ensemble_comparison():\n",
    "    \"\"\"\n",
    "    Compare multiple ensemble methods on the same dataset using cross-validation.\n",
    "    \n",
    "    Methods: Random Forest, Gradient Boosting, Voting, Stacking\n",
    "    \n",
    "    Returns:\n",
    "        dict: Cross-validation results for each ensemble method\n",
    "    \"\"\"\n",
    "    # Load and prepare data  \n",
    "    cancer = load_breast_cancer()\n",
    "    X, y = cancer.data, cancer.target\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # TODO: Define ensemble models\n",
    "    models = {}\n",
    "    \n",
    "    # 1. Random Forest\n",
    "    models['Random Forest'] = None  # RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # 2. Gradient Boosting  \n",
    "    models['Gradient Boosting'] = None  # GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # 3. Voting Classifier\n",
    "    base_models = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
    "        ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "        ('svm', SVC(random_state=42, probability=True))\n",
    "    ]\n",
    "    models['Voting'] = None  # VotingClassifier(estimators=base_models, voting='soft')\n",
    "    \n",
    "    # 4. Stacking Classifier\n",
    "    stacking_base = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=50, random_state=42))\n",
    "    ]\n",
    "    models['Stacking'] = None  # StackingClassifier(estimators=stacking_base, final_estimator=LogisticRegression(), cv=3)\n",
    "    \n",
    "    # TODO: Calculate cross-validation scores for each model\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        if model is not None:\n",
    "            # Use cross_val_score with cv=5\n",
    "            cv_scores = None  # cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "            results[name] = {\n",
    "                'scores': cv_scores,\n",
    "                'mean': np.mean(cv_scores) if cv_scores is not None else None,\n",
    "                'std': np.std(cv_scores) if cv_scores is not None else None\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "@validator.koan(7, \"Ensemble Method Comparison\", difficulty=\"Advanced\")\n",
    "def validate():\n",
    "    results = comprehensive_ensemble_comparison()\n",
    "    \n",
    "    assert results is not None, \"Results is None\"\n",
    "    assert len(results) == 4, f\"Should have 4 ensemble methods, got {len(results)}\"\n",
    "    \n",
    "    expected_methods = ['Random Forest', 'Gradient Boosting', 'Voting', 'Stacking']\n",
    "    for method in expected_methods:\n",
    "        assert method in results, f\"Missing {method} in results\"\n",
    "        assert results[method]['scores'] is not None, f\"{method} scores is None\"\n",
    "        assert len(results[method]['scores']) == 5, f\"{method} should have 5 CV scores\"\n",
    "        assert 0.7 <= results[method]['mean'] <= 1.0, f\"{method} mean score should be reasonable\"\n",
    "    \n",
    "    print(\"‚úì Successfully compared all ensemble methods!\")\n",
    "    print(f\"\\n  üìä Cross-Validation Results (5-fold):\")\n",
    "    print(f\"  {'Method':<18} {'Mean':<8} {'Std':<8} {'Range'}\")\n",
    "    print(f\"  {'-'*50}\")\n",
    "    \n",
    "    # Sort by mean performance\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
    "    \n",
    "    for i, (method, result) in enumerate(sorted_results):\n",
    "        mean = result['mean']\n",
    "        std = result['std'] \n",
    "        min_score = np.min(result['scores'])\n",
    "        max_score = np.max(result['scores'])\n",
    "        \n",
    "        print(f\"  {method:<18} {mean:.3f}    {std:.3f}    [{min_score:.3f}, {max_score:.3f}]\")\n",
    "    \n",
    "    # Identify best performer\n",
    "    best_method, best_result = sorted_results[0]\n",
    "    print(f\"\\n  üèÜ Best Performer: {best_method} ({best_result['mean']:.3f} ¬± {best_result['std']:.3f})\")\n",
    "    \n",
    "    # Performance insights\n",
    "    print(f\"\\n  üí° Ensemble Method Characteristics:\")\n",
    "    print(f\"    üå≤ Random Forest: Fast, interpretable, good baseline\")\n",
    "    print(f\"    üöÄ Gradient Boosting: Sequential learning, can overfit\") \n",
    "    print(f\"    üó≥Ô∏è  Voting: Combines diverse algorithms, robust\")\n",
    "    print(f\"    ü•û Stacking: Meta-learning, most flexible\")\n",
    "    \n",
    "    print(f\"\\n  üéØ Choosing the Right Ensemble:\")\n",
    "    print(f\"    ‚Ä¢ Speed needed? ‚Üí Random Forest\")\n",
    "    print(f\"    ‚Ä¢ Maximum accuracy? ‚Üí Stacking or XGBoost\") \n",
    "    print(f\"    ‚Ä¢ Interpretability? ‚Üí Random Forest + feature importance\")\n",
    "    print(f\"    ‚Ä¢ Robustness? ‚Üí Voting with diverse models\")\n",
    "\n",
    "validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You have mastered ensemble methods - some of the most powerful techniques in machine learning!\n",
    "\n",
    "### What You've Accomplished\n",
    "- ‚úÖ **Random Forest**: Built bagging ensembles with bootstrap sampling\n",
    "- ‚úÖ **Feature Importance**: Analyzed variable significance in tree ensembles  \n",
    "- ‚úÖ **Gradient Boosting**: Implemented sequential learning algorithms\n",
    "- ‚úÖ **XGBoost**: Used state-of-the-art optimized boosting\n",
    "- ‚úÖ **Voting Classifiers**: Combined diverse algorithms effectively\n",
    "- ‚úÖ **Stacking**: Implemented meta-learning for optimal combination\n",
    "- ‚úÖ **Ensemble Comparison**: Evaluated trade-offs between methods\n",
    "\n",
    "### Key Insights Gained\n",
    "1. **Ensemble Power**: Multiple weak learners create strong predictors\n",
    "2. **Diversity Matters**: Different algorithm types improve ensemble performance\n",
    "3. **Bias-Variance Trade-off**: Bagging reduces variance, boosting reduces bias\n",
    "4. **Meta-Learning**: Stacking can learn optimal combination strategies\n",
    "5. **Real-World Impact**: Ensembles dominate ML competitions and applications\n",
    "\n",
    "### Next Steps  \n",
    "- **Notebook 13**: Hyperparameter Tuning (optimize your ensembles!)\n",
    "- **Advanced Topics**: Deep ensemble methods, neural network ensembles\n",
    "- **Practice**: Apply ensembles to your own datasets and competitions\n",
    "\n",
    "### Real-World Applications\n",
    "- **Finance**: Credit scoring, algorithmic trading, fraud detection\n",
    "- **Healthcare**: Disease diagnosis, drug discovery, medical imaging\n",
    "- **Technology**: Recommendation systems, search ranking, ad targeting  \n",
    "- **Science**: Climate modeling, genomics, particle physics\n",
    "- **Business**: Customer churn, demand forecasting, price optimization\n",
    "\n",
    "You now have the tools to build world-class predictive models! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Progress Check\n",
    "progress = tracker.get_notebook_progress('12_ensemble_methods')\n",
    "print(f\"\\nüìä Your Progress: {progress}% complete!\")\n",
    "\n",
    "if progress == 100:\n",
    "    print(\"üéâ Outstanding! You've mastered all ensemble method koans!\")\n",
    "    print(\"üéØ Ready for Notebook 13: Hyperparameter Tuning\")\n",
    "elif progress >= 75:\n",
    "    print(\"üåü Excellent progress! Almost there with ensemble mastery.\")\n",
    "elif progress >= 50:\n",
    "    print(\"üí™ Great work! You're building powerful ensemble skills.\")\n",
    "else:\n",
    "    print(\"üöÄ Keep going! Each ensemble technique builds on the last.\")\n",
    "\n",
    "print(f\"\\nüìà Overall course progress:\")\n",
    "total_notebooks = 15\n",
    "completed_notebooks = len([nb for nb in range(1, 13) if tracker.get_notebook_progress(f'{nb:02d}_*') == 100])\n",
    "print(f\"   Completed notebooks: {completed_notebooks}/{total_notebooks}\")\n",
    "print(f\"   Course progress: {(completed_notebooks/total_notebooks)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüéØ Ensemble Method Mastery Achieved!\")\n",
    "print(f\"   You can now build production-ready ML ensembles! üèÜ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
